{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf630c4",
   "metadata": {
    "id": "abf630c4"
   },
   "source": [
    "# Transformeurs visuels (ViT) avec PyTorch\n",
    "\n",
    "Dans ce cahier, nous allons compléter l'exemple commencé en classe au dernier cours.\n",
    "Le but est d'apprendre à importer un modèle pré-défini et pré-entraîné dans PyTorch pour l'utiliser sur de nouvelles données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbce2c2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1fbce2c2",
    "outputId": "a3d9e6b5-96a7-403a-b5d0-16e17fcba857"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf39f3",
   "metadata": {
    "id": "0bdf39f3"
   },
   "source": [
    "## Importation du modèle\n",
    "\n",
    "Le module `torchvision` de PyTorch implémente différent modèles pré-entraînés, incluant des [ViT](https://pytorch.org/vision/main/models/vision_transformer.html).\n",
    "On peut directement importer le plus petit des modèles disponibles avec `vit_b_16`.\n",
    "Cette fonction permet de créer un modèle complet lorsqu'elle est appelée.\n",
    "On peut spécifier `weights=\"DEFAULT\"` pour utiliser des poids pré-entraînés.\n",
    "Tel que mentionné dans [la documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights), ces poids sont entraînés sur les données ImageNet-1K.\n",
    "On peut aussi utiliser l'interface `ViT_B_16_Weights`, qui est intéressante car elle donne accès à une `transform` PyTorch pour convertir les données au format attendu (voir plus bas lors de la création de l'ensemble de données), ainsi qu'aux classes des données ImageNet.\n",
    "\n",
    "**Exercice: Importer un modèle VIT pré-entraîné avec PyTorch, comptez le nombre de paramètres et affichez son architecture complète.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4ef8e",
   "metadata": {
    "id": "52c4ef8e"
   },
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791c8e3",
   "metadata": {
    "id": "5791c8e3"
   },
   "outputs": [],
   "source": [
    "# TODO: Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "W5iCvBCm07Le",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W5iCvBCm07Le",
    "outputId": "2d751b85-b648-4b6d-e8bd-a49587dfe16b"
   },
   "outputs": [],
   "source": [
    "parameters = list(model.parameters())\n",
    "parameters[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df976c6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6df976c6",
    "outputId": "ccb6991e-fef2-48f8-95af-debe5e20bfad"
   },
   "outputs": [],
   "source": [
    "# TODO: Count parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2f9fd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1ad2f9fd",
    "outputId": "719ea6d6-0224-44b8-f8c2-383fea439673"
   },
   "outputs": [],
   "source": [
    "# TODO: Afficher le modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba2357",
   "metadata": {
    "id": "63ba2357"
   },
   "source": [
    "On retrouve bien les éléments que nous avons vu dans le cours:\n",
    "\n",
    "- 12 blocs d'encodeurs\n",
    "- Une attention multi-tête\n",
    "- Un bloc pleinement connecté\n",
    "- Une normalisation de couche (`LayerNorm`)\n",
    "- Une « tête » de classification à la fin du modèle\n",
    "\n",
    "Pour comprendre comment les couches sont sont implémentées, on peut se référer directement au [code source](https://pytorch.org/vision/main/_modules/torchvision/models/vision_transformer.html#vit_b_16).\n",
    "\n",
    "Remarquez que comme les données ImageNet ont 1000 classes, il y a 1000 sorties. On peut d'ailleurs accéder aux classes via les poids.\n",
    "\n",
    "**Exercice: Utilisez pretrained_weights pour accéder aux classes des données ImageNet.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e14b5",
   "metadata": {
    "id": "0e6e14b5"
   },
   "outputs": [],
   "source": [
    "# TODO: Utilisez les poids pré-entraînés pour accéder aux classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3ed74",
   "metadata": {
    "id": "37d3ed74"
   },
   "source": [
    "## Importation des données\n",
    "\n",
    "Comme dans les exemples vus plus tôt cette session, les données doivent être formattées dans un `Dataset` PyTorch.\n",
    "Malheureusement, les données ImageNet utilisées pour entraîner le réseau sont très volumineuses.\n",
    "Pour sauver du temps ici, nous utiliserons un ensemble de données déjà disponible dans PyTorch, soit l'ensemble [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Comme le ViT s'attend à des images de 224x224 pixels, on convertit les images vers cette taille.\n",
    "On pourrait le faire avec `torchvision.transforms` directement, mais les poids pré-entraînés donnent accès aux transformations permettant de rendre les données compatibles avec le modèle, soit:\n",
    "\n",
    "- Une interpolation pour ajuster la taille des images 256x256\n",
    "- Une découpure centrale de 224x224\n",
    "- Une mise à l'échelle entre 0 et 1\n",
    "- Un normalisation avec `mean=[0.485, 0.456, 0.406]` et `std=[0.229, 0.224, 0.225]`.\n",
    "\n",
    "J'ai inclus une implémentation manuelle, mais on peut utiliser les poids directement pour éviter les erreurs.\n",
    "\n",
    "**Exercice: Utilisez torchvision pour implémenter manuellement la transformation décrite ci-dessus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23e41a",
   "metadata": {
    "id": "0e23e41a"
   },
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize, ToTensor, Compose, Resize, CenterCrop\n",
    "\n",
    "transform_type = \"weights\"\n",
    "\n",
    "if transform_type == \"manual\":\n",
    "    # TODO: Manual implementation\n",
    "    pass\n",
    "else:\n",
    "    transforms = pretrained_weights.transforms()\n",
    "train_dataset = torchvision.datasets.CIFAR10('../cifar10',  download=True, train=True, transform=transforms)\n",
    "test_dataset = torchvision.datasets.CIFAR10('../cifar10',  download=True, train=False, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Z3lpQb414HK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Z3lpQb414HK",
    "outputId": "34261060-bf7f-4a35-a619-d35ec8718d25"
   },
   "outputs": [],
   "source": [
    "train_dataset[0][0].device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595ecd1b-2ae6-47a9-acba-704348892443",
   "metadata": {},
   "source": [
    "Les données CIFAR10 nous donnent un dictionnaire pour convertir le nom de chaque classe vers une indice.\n",
    "\n",
    "**Exercice: Créez le dictionnaire inverse à `label2idx`, soit `idx2label`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b222df",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38b222df",
    "outputId": "bf242aa7-035a-408b-ec1c-df112571408d"
   },
   "outputs": [],
   "source": [
    "print(\"Classes de CIFAR10:\", train_dataset.class_to_idx)\n",
    "labels = list(train_dataset.class_to_idx)\n",
    "label2idx = train_dataset.class_to_idx\n",
    "# TODO: idx2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f8c77b-c460-4599-9cf5-3b927b37db1d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "314adedf",
    "outputId": "a5bdcab6-1e1b-45be-9072-438bc4e22e8f"
   },
   "outputs": [],
   "source": [
    "print(\"Nombre d'images d'entraînement:\", len(train_dataset))\n",
    "print(\"Nombre d'images de test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636281c0-8369-47db-8c67-27fbc9c0b3d7",
   "metadata": {},
   "source": [
    "Ça fait beaucoup de données! Pour éviter que le TP ne prenne trop de temps, on peut se limiter à une fraction des exemples.\n",
    "\n",
    "**Exercice: Gardez 10% des données test et 10% des données d'entraînement. Je suggère d'utiliser `random_split`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SNopxzzE6Api",
   "metadata": {
    "id": "SNopxzzE6Api"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# TODO: Keep 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a078d16-6dbf-41e1-917d-3af2cec586b4",
   "metadata": {},
   "source": [
    "**Exercice: affichez un exemple aléatoire tiré des données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6d49c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "ead6d49c",
    "outputId": "9be2dc6c-117c-440a-9ccf-c39a42f64f56"
   },
   "outputs": [],
   "source": [
    "idx = torch.randint(len(train_dataset), ())\n",
    "img, label = train_dataset[idx]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(idx2label[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7222c",
   "metadata": {
    "id": "14a7222c"
   },
   "source": [
    "Remarquez les couleurs étranges. C'est dû aux transformations effectuées ci-dessus. Vous pouvez tester la version manuelle et enlever la normalisation pour le confirmer.\n",
    "\n",
    "## Test du modèle\n",
    "\n",
    "Comme le modèle est pré-entraîné, on peut s'attendre à une certaine qualité dans les prédictions.\n",
    "Cependant, nos données ne sont pas les mêmes que celles utilisées à l'entraînement.\n",
    "Le nom des classes n'est donc pas identique, et il se peut que des classes de CIFAR10 ne se trouvent pas dans ImageNet.\n",
    "On peut tout de même extraire les classes ImageNet ayant la meilleure probabilité et les comparer avec la vraie classe de l'objet.\n",
    "\n",
    "**Exercice: Tirez un exemple aléatoire et affichez la probabilité des 5 classes ImageNet les plus probables selon le ViT. Affichez également la vraie classe CIFAR10.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa49893",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "id": "8aa49893",
    "outputId": "31a8a3fc-93e9-44b0-d693-078c268614de"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pprint\n",
    "\n",
    "# TODO: Classe et affichage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1477ffe",
   "metadata": {
    "id": "d1477ffe"
   },
   "source": [
    "En répétant la cellule ci-dessus quelques fois, on voit que la prédiction n'est pas si mauvaise dans plusieurs cas.\n",
    "\n",
    "## Entraînement\n",
    "\n",
    "Dans l'espoir d'améliorer la performance modèle, on peut effectuer un entraînement spécifiquement pour les données CIFAR10.\n",
    "Comme le modèle est pré-entraîné, on nomme souvent cet entraînement _fine tuning_.\n",
    "Pour ce faire il faudra:\n",
    "\n",
    "- Remplacer la tête de classification pour qu'elle ait 10 catégories de sortie.\n",
    "- Choisir quels paramètres entraîner. On pourra « geler » le ViT et seulement entraîner le classificateur, ou bien entraîner tout le modèle.\n",
    "- Définir une boucle d'entraînement comme nous l'avons fait dans les TPs précédents.\n",
    "\n",
    "### Remplacement du classificateur\n",
    "\n",
    "Pour remplacer le classificateur, voyons voir comment ce dernier est implémenté dans le modèle initial.\n",
    "\n",
    "**Exercice: Explorez la structure du modèle et remplacez la tête de classification.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131a4f0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d131a4f0",
    "outputId": "e4c3a6f9-5d55-49fb-ff49-238701baafa4"
   },
   "outputs": [],
   "source": [
    "# TODO: Modification du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f9efd",
   "metadata": {
    "id": "b87f9efd"
   },
   "source": [
    "Cependant, comme cette nouvelle couche n'est pas entraînée, les prédictions ne seront pas bonnes.\n",
    "C'est pourquoi il faut ajuster le modèle.\n",
    "\n",
    "### Boucle d'entraînement\n",
    "\n",
    "La boucle d'entraînement fonctionne comme dans les TPs précédent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba25b8",
   "metadata": {
    "id": "b4ba25b8"
   },
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab80f0a",
   "metadata": {
    "id": "0ab80f0a"
   },
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    # Taille de l'ensemble d'entraînement\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Itération sur les batches (dataloader nous donne les données par batch)\n",
    "    # X est l'image et y la classe\n",
    "    train_loss = 0.0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Fonction objectif\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(X)  # prédiction\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Rétropropagation\n",
    "        optimizer.zero_grad()  # On réinitialise le gradient d'abord\n",
    "        loss.backward()  # Rétropropagation\n",
    "        optimizer.step()  # On fait un pas dans l'espace paramètre\n",
    "\n",
    "        loss, current = loss.item(), (batch+1) * len(X)\n",
    "        train_loss += loss * X.size(0)\n",
    "        # Progrès\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss: {loss}, [{current}/{size}]\")\n",
    "\n",
    "    return train_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97068020",
   "metadata": {
    "id": "97068020"
   },
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = correct = 0\n",
    "\n",
    "    # On se sauve les gradients comme ils ne sont pas utilisés\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()  # Compute loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct)}%, Avg loss: {test_loss} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3219b",
   "metadata": {
    "id": "4dc3219b"
   },
   "source": [
    "Préparons maintenant les données avec les classes `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48eccf",
   "metadata": {
    "id": "0e48eccf",
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                        shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9cb866",
   "metadata": {
    "id": "3b9cb866"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65707748",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "65707748",
    "outputId": "3231f0d7-eeb6-4f90-f74a-578bbf9651a1"
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    model.eval()\n",
    "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5061a72-9cee-4261-bec6-99e55a6a2573",
   "metadata": {},
   "source": [
    "## Exercices additionnels\n",
    "\n",
    "- Testez le notebook avec un autre ensemble de données. Je suggère GalaxyMNIST, qui se rapproche du devoir 3. Vous pourriez également tester les données du devoir 3 en remplaçant la tête de classification par une régression!\n",
    "- Testez le notebook avec un autre ViT, soit L ou H au lieu de B\n",
    "- Testez le notebook avec un autre modèle PyTorch, soit ResNet, ConvNeXt ou autre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f704e-5751-462e-9b1a-6ab89b2a294c",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "Il existe énormément de références sur les transformeurs. En voici quelques unes:\n",
    "\n",
    "- Chapitre d'un livre en ligne sur ViT: <https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html>\n",
    "- Implémentation de plusieurs transformers en PyTorch: <https://github.com/lucidrains/vit-pytorch>"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
