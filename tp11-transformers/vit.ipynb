{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abf630c4",
   "metadata": {},
   "source": [
    "# Transformeurs visuels (ViT) avec PyTorch\n",
    "\n",
    "Dans ce cahier, nous allons compléter l'exemple commencé en classe au dernier cours.\n",
    "Le but est d'apprendre à importer un modèle pré-défini et pré-entraîné dans PyTorch pour l'utiliser sur de nouvelles données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbce2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdf39f3",
   "metadata": {},
   "source": [
    "## Importation du modèle\n",
    "\n",
    "Le module `torchvision` de PyTorch implémente différent modèles pré-entraînés, incluant des [ViT](https://pytorch.org/vision/main/models/vision_transformer.html).\n",
    "On peut directement importer le plus petit des modèles disponibles avec `vit_b_16`.\n",
    "Cette fonction permet de créer un modèle complet lorsqu'elle est appelée.\n",
    "On peut spécifier `weights=\"DEFAULT\"` pour utiliser des poids pré-entraînés.\n",
    "Tel que mentionné dans [la documentation](https://pytorch.org/vision/main/models/generated/torchvision.models.vit_b_16.html#torchvision.models.ViT_B_16_Weights), ces poids sont entraînés sur les données ImageNet-1K.\n",
    "On peut aussi utiliser l'interface `ViT_B_16_Weights`, qui est intéressante car elle donne accès à une `transform` PyTorch pour convertir les données au format attendu (voir plus bas lors de la création de l'ensemble de données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c4ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import vit_b_16, ViT_B_16_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5791c8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_weights = ViT_B_16_Weights.DEFAULT\n",
    "model = vit_b_16(weights=pretrained_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feeeee7",
   "metadata": {},
   "source": [
    "Commençons par compter les paramètres du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df976c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of parameters\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7af442",
   "metadata": {},
   "source": [
    "On peut aussi visualiser l'architecture du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad2f9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ba2357",
   "metadata": {},
   "source": [
    "On retrouve bien les éléments que nous avons vu dans le cours:\n",
    "\n",
    "- 12 blocs d'encodeurs\n",
    "- Une attention multi-tête\n",
    "- Un bloc pleinement connecté\n",
    "- Une normalisation de couche (`LayerNorm`)\n",
    "- Une « tête » de classification à la fin du modèle\n",
    "\n",
    "Pour comprendre comment les couches sont sont implémentées, on peut se référer directement au [code source](https://pytorch.org/vision/main/_modules/torchvision/models/vision_transformer.html#vit_b_16).\n",
    "\n",
    "Remarquez que comme les données ImageNet ont 1000 classes, il y a 1000 sorties. On peut d'ailleurs accéder aux classes via les poids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6e14b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1k = pretrained_weights.meta[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d3ed74",
   "metadata": {},
   "source": [
    "## Importation des données\n",
    "\n",
    "Comme dans les exemples vus plus tôt cette session, les données doivent être formattées dans un `Dataset` PyTorch.\n",
    "Malheureusement, les données ImageNet utilisées pour entraîner le réseau sont très volumineuses.\n",
    "Pour sauver du temps ici, nous utiliserons un ensemble de données déjà disponible dans PyTorch, soit l'ensemble [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html).\n",
    "\n",
    "Comme le ViT s'attend à des images de 224x224 pixels, on convertit les images vers cette taille.\n",
    "On pourrait le faire avec `torchvision.transforms` directement, mais les poids pré-entraînés donnent accès aux transformations permettant de rendre les données compatibles avec le modèle, soit:\n",
    "\n",
    "- Une interpolation pour ajuster la taille des images 256x256\n",
    "- Une découpure centrale de 224x224\n",
    "- Une mise à l'échelle entre 0 et 1\n",
    "- Un normalisation avec `mean=[0.485, 0.456, 0.406]` et `std=[0.229, 0.224, 0.225]`.\n",
    "\n",
    "J'ai inclus une implémentation manuelle, mais on peut utiliser les poids directement pour éviter les erreurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e23e41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import Normalize, ToTensor, Compose, Resize, CenterCrop\n",
    "\n",
    "transform_type = \"weights\"\n",
    "\n",
    "if transform_type == \"manual\":\n",
    "    transforms = Compose([\n",
    "        Resize((256, 256)),\n",
    "        CenterCrop((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "else:\n",
    "    transforms = pretrained_weights.transforms()\n",
    "train_dataset = torchvision.datasets.CIFAR10('../cifar10',  download=True, train=True, transform=transforms)\n",
    "test_dataset = torchvision.datasets.CIFAR10('../cifar10',  download=True, train=False, transform=transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b222df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Classes de CIFAR10:\", train_dataset.class_to_idx)\n",
    "labels = list(train_dataset.class_to_idx)\n",
    "label2idx = train_dataset.class_to_idx\n",
    "idx2label = dict(zip(label2idx.values(), label2idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314adedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Nombre d'images d'entraînement:\", len(train_dataset))\n",
    "print(\"Nombre d'images de test:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead6d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = torch.randint(len(train_dataset), ())\n",
    "img, label = train_dataset[idx]\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(idx2label[label])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a7222c",
   "metadata": {},
   "source": [
    "Remarquez les couleurs étranges. C'est dû aux transformations effectuées ci-dessus. Vous pouvez tester la version manuelle et enlever la normalisation pour le confirmer.\n",
    "\n",
    "## Test du modèle\n",
    "\n",
    "Comme le modèle est pré-entraîné, on peut s'attendre à une certaine qualité dans les prédictions.\n",
    "Cependant, nos données ne sont pas les mêmes que celles utilisées à l'entraînement.\n",
    "Le nom des classes n'est donc pas identique, et il se peut que des classes de CIFAR10 ne se trouvent pas dans ImageNet.\n",
    "On peut tout de même extraire les classes ImageNet ayant la meilleure probabilité et les comparer avec la vraie classe de l'objet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa49893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import pprint\n",
    "\n",
    "idx = torch.randint(len(train_dataset), ())\n",
    "img, label_idx = train_dataset[idx]\n",
    "label = idx2label[label_idx]\n",
    "pred1k = model(img.unsqueeze(0))\n",
    "\n",
    "plt.imshow(img.permute(1, 2, 0))\n",
    "plt.title(label)\n",
    "plt.show()\n",
    "\n",
    "prob1k = F.softmax(pred1k, dim=1)\n",
    "n_best = 5\n",
    "idx_best = torch.argsort(prob1k, dim=-1, descending=True)[:, :n_best].squeeze()\n",
    "labels_best = [labels1k[idx] for idx in idx_best]\n",
    "n_best_dict = dict(zip(labels_best, prob1k[:, idx_best].squeeze().tolist()))\n",
    "print(\"True CIFAR10 label:\", label)\n",
    "print(\"Best image1k labels:\")\n",
    "pprint.pprint(n_best_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1477ffe",
   "metadata": {},
   "source": [
    "En répétant la cellule ci-dessus quelques fois, on voit que la prédiction n'est pas si mauvaise dans plusieurs cas.\n",
    "\n",
    "## Entraînement\n",
    "\n",
    "Dans l'espoir d'améliorer la performance modèle, on peut effectuer un entraînement spécifiquement pour les données CIFAR10.\n",
    "Comme le modèle est pré-entraîné, on nomme souvent cet entraînement _fine tuning_.\n",
    "Pour ce faire il faudra:\n",
    "\n",
    "- Remplacer la tête de classification pour qu'elle ait 10 catégories de sortie.\n",
    "- Choisir quels paramètres entraîner. On pourra « geler » le ViT et seulement entraîner le classificateur, ou bien entraîner tout le modèle.\n",
    "- Définir une boucle d'entraînement comme nous l'avons fait dans les TPs précédents.\n",
    "\n",
    "### Remplacement du classificateur\n",
    "\n",
    "Pour remplacer le classificateur, voyons voir comment ce dernier est implémenté dans le modèle initial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1ee8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97784e26",
   "metadata": {},
   "source": [
    "On voit que la tête de classification (`heads`) est définie avec `nn.Linear`. On peut y accéder comme suit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d131a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.heads.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d769de5",
   "metadata": {},
   "source": [
    "Pour obtenir 10 classes au lieu de 1000, il suffit de réassigner cet élément avec une nouvelle couche linéaire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2f04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model.heads.head = nn.Linear(768, out_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ff484d",
   "metadata": {},
   "source": [
    "Vérifions que le changement s'est bien fait:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e746be",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15c1871",
   "metadata": {},
   "source": [
    "et que le modèle prédit en effet 10 classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18652e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(img.unsqueeze(0))\n",
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87f9efd",
   "metadata": {},
   "source": [
    "Cependant, comme cette nouvelle couche n'est pas entraînée, les prédictions ne seront pas bonnes.\n",
    "C'est pourquoi il faut ajuster le modèle.\n",
    "\n",
    "### Boucle d'entraînement\n",
    "\n",
    "La boucle d'entraînement fonctionne comme dans les TPs précéðent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ba25b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab80f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "\n",
    "    # Taille de l'ensemble d'entraînement\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    # Itération sur les batches (dataloader nous donne les données par batch)\n",
    "    # X est l'image et y la classe\n",
    "    train_loss = 0.0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Fonction objectif\n",
    "        pred = model(X)  # prédiction\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Rétropropagation\n",
    "        optimizer.zero_grad()  # On réinitialise le gradient d'abord\n",
    "        loss.backward()  # Rétropropagation\n",
    "        optimizer.step()  # On fait un pas dans l'espace paramètre\n",
    "\n",
    "        loss, current = loss.item(), (batch+1) * len(X)\n",
    "        train_loss += loss * X.size(0)\n",
    "        # Progrès\n",
    "        if batch % 100 == 0:\n",
    "            print(f\"Loss: {loss}, [{current}/{size}]\")\n",
    "\n",
    "    return train_loss / size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97068020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = correct = 0\n",
    "\n",
    "    # On se sauve les gradients comme ils ne sont pas utilisés\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()  # Compute loss\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct)}%, Avg loss: {test_loss} \\n\")\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3219b",
   "metadata": {},
   "source": [
    "Préparons maintenant les données avec les classes `DataLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48eccf",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9cb866",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65707748",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    model.train()\n",
    "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    train_losses.append(train_loss)\n",
    "    model.eval()\n",
    "    test_loss = test_loop(test_dataloader, model, loss_fn)\n",
    "    test_losses.append(test_loss)\n",
    "print(\"Done!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
