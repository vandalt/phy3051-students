{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vandalt/phy3051-students-private/blob/main/tp11-transformers/huggingface_blank.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c072b4e4",
      "metadata": {
        "id": "c072b4e4"
      },
      "source": [
        "# Huggingface\n",
        "\n",
        "Dans le notebook précédent, nous avons vu comment un transformer peut-être créé et modifié directement avec PyTorch.\n",
        "On aurait également pu aller un pas plus loin et implémenter le modèle nous même avec PyTorch.\n",
        "C'est un exercice intéressant, mais qui demande un peu plus de temps que ce que nous avons dans les TPs.\n",
        "Ceci dit, je vous encourage à consulter des ressources en ligne à ce sujet si ça vous intéresse (voir le notebook PyTorch).\n",
        "\n",
        "Ici, nous explorerons plutôt [_huggingface_](https://huggingface.co/).\n",
        "Huggingface est un peu comme un GitHub pour les modèles d'IA.\n",
        "Les utilisateurs peuvent publier l'architecture de leur modèle, des poids pré-entraînés et des ensembles de données.\n",
        "Le site fourni aussi des librairies implémentant certains communs, notamment des [transformers](https://huggingface.co/docs/transformers/index) et des [modèles de diffusion](https://huggingface.co/docs/diffusers/index).\n",
        "Voir la section [documentation](https://huggingface.co/docs) pour plus de détails.\n",
        "Dans ce notebook, nous allons nous familiariser avec la librairie [transformers](https://huggingface.co/docs/transformers/index).\n",
        "\n",
        "La librairie `transformers` va un peu dans la direction opposée d'une implémentation complète en PyTorch: presque toutes les opérations sont cachées derrières des classes nous permettant simplement de spécifier les paramètres de notre modèle.\n",
        "Ce n'est pas la meilleure façon de comprendre tous les détails d'un modèle, mais c'est pratique pour le tester rapidement et comprendre comment il interprète les données.\n",
        "De plus, tous les modèles de la librairie sont disponible [sur GitHub](https://github.com/huggingface/transformers/tree/main/src/transformers/models).\n",
        "N'hésitez pas à les consulter!\n",
        "\n",
        "## Installation\n",
        "\n",
        "Pour utiliser `transformers`, il faudra d'abord l'installer. Nous installerons du même coup les autres librries huggingface utilisées dans ce TP."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3706a0cd",
      "metadata": {
        "id": "3706a0cd"
      },
      "outputs": [],
      "source": [
        "INSTALL = False\n",
        "if INSTALL:\n",
        "    !python -m pip -q install transformers datasets evaluate\n",
        "else:\n",
        "    print(\"Skip install\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04c4ab00",
      "metadata": {
        "id": "04c4ab00"
      },
      "source": [
        "Si vous n'avez pas de GPU, vous pouvez remplacer `transformers` par `transformers[torch]`.\n",
        "\n",
        "## Connexion\n",
        "\n",
        "L'accès à certains modèles Huggingface requiert un compte et une authentification.\n",
        "Pour se connecter, on peut utiliser `notebook_login()` dans un notebook et `login()` dans un terminal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e757ce12",
      "metadata": {
        "id": "e757ce12"
      },
      "outputs": [],
      "source": [
        "def is_notebook() -> bool:\n",
        "    # https://stackoverflow.com/questions/15411967/how-can-i-check-if-code-is-executed-in-the-ipython-notebook\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False      # Probably standard Python interpreter\n",
        "\n",
        "NOTEBOOK = is_notebook()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd1762a",
      "metadata": {
        "id": "2cd1762a"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, login\n",
        "\n",
        "if NOTEBOOK:\n",
        "    notebook_login()\n",
        "else:\n",
        "    login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0abc640e",
      "metadata": {
        "id": "0abc640e"
      },
      "source": [
        "## Pipelines\n",
        "\n",
        "L'interface la plus simple de `transformers` est la classe `Pipeline`.\n",
        "Celle-ci nous permet d'importer et d'utiliser un transformeur en trois lignes de code!\n",
        "\n",
        "### Génération de texte\n",
        "\n",
        "Bien que ce ne soit pas de l'analyse de données physique, la génération de texte est tellement omni-présente dans les dernières années qu'il peut être intéressant de voir comment l'appliquer avec Huggingface.\n",
        "\n",
        "Le modèle [Gemma](https://huggingface.co/google/gemma-3-1b-it) de Google requiert une authentification avec les cellules ci-dessus. Si vous ne souhaitez pas vous authentifier, décommentez la deuxième ligne. C'est GPT-2 qui est utilisé par défaut.\n",
        "\n",
        "J'ai également inclut deux exemples. L'un qui simule un chat-bot et l'autre qui demande simplement de compléter une phrase. Les deux fonctionnent avec Gemma, mais seulement le 2e avec GPT-2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e3da893",
      "metadata": {
        "id": "0e3da893"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "txt_pipeline = pipeline(task=\"text-generation\", model=\"google/gemma-3-1b-it\")\n",
        "# txt_pipeline = pipeline(task=\"text-generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982535f4",
      "metadata": {
        "id": "982535f4"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},]\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": \"What is Markov Chain Monte Carlo? Explain in two sentences.\"},]\n",
        "        },\n",
        "    ],\n",
        "]\n",
        "# messages = \"Markov Chain Monte Carlo is an inference method that\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "285138eb",
      "metadata": {
        "id": "285138eb"
      },
      "outputs": [],
      "source": [
        "simple = isinstance(messages, str)\n",
        "reply = txt_pipeline(messages, max_new_tokens=100)\n",
        "if simple:\n",
        "    print(reply[0][\"generated_text\"])\n",
        "else:\n",
        "    print(reply[0][0][\"generated_text\"][2][\"content\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dkEKJmUDNpOF",
      "metadata": {
        "id": "dkEKJmUDNpOF"
      },
      "source": [
        "Ces modèles sont un peu volumineux. On peut simplement le supprimer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e26b1aeb",
      "metadata": {
        "id": "e26b1aeb"
      },
      "outputs": [],
      "source": [
        "del txt_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9412fc22",
      "metadata": {
        "id": "9412fc22"
      },
      "source": [
        "### Classification d'image\n",
        "\n",
        "L'interface `pipeline` ne se limite bien sûr pas à la génération de texte.\n",
        "On peut spécifier une autre tâche via le premier argument, `task`.\n",
        "Par exemple, pour classifier des images on utiliserait `task=\"image-classification\"`.\n",
        "Le modèle par défaut est le transformeur visuel `vit` avec des sous-images de 16 et une taille initiale de 224x224 pixels, soit ([google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)).\n",
        "On spécifie l'argument `model` ci-dessous pour clarifier le modèle utilisé.\n",
        "\n",
        "**Exercice: Créez un pipeline destiné à la classification d'image et appliquez le à n'importe quelle image trouvée en ligne. Il suffit de passer le lien en argument au pipeline. N'oubliez pas de supprimer le pipeline ensuite.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f8f3270-386f-4415-a6bf-559ec6c7452f",
      "metadata": {
        "id": "7f8f3270-386f-4415-a6bf-559ec6c7452f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c6fa7d62",
      "metadata": {
        "id": "c6fa7d62"
      },
      "source": [
        "## Interface complète\n",
        "\n",
        "Le `pipeline` ci-dessus nous permet de tester un modèle très rapidement, mais ne permet pas d'interagir avec un ensemble de données ou d'entraîner le modèle.\n",
        "\n",
        "### Importation des données\n",
        "\n",
        "Comme avec PyTorch, huggingface comprends plusieurs ensembles de données.\n",
        "Pour y accéder, on peut utiliser la librairie [Datasets](https://huggingface.co/docs/datasets/index).\n",
        "\n",
        "Ici, on importe seulement les 5000 premiers exemples des données d'entraînement pour réduire la taille des fichiers sur notre disque.\n",
        "On pourra créer nos propre sous-ensembles à partir des données d'entraînement uniquement.\n",
        "\n",
        "**Exercice: Explorez les datasets Huggingface et choisissez en un pour cet exemple. Vous pouvez utiliser `split=\"train[:5000]\"` pour importer uniquement 5000 exemples.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99812d72",
      "metadata": {
        "id": "99812d72"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# TODO: Import dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6df07540",
      "metadata": {
        "id": "6df07540"
      },
      "source": [
        "On voit que les données contiennent des images et leurs annotation.\n",
        "Séparons maintenant le tout avec 80% des exemples utilisés dans l'entraînement et le dernier 20% utilisé pour la validation.\n",
        "\n",
        "**Exercice: Utilisez la méthode `train_test_split` pour garder 80% des données dans l'ensemble d'entraînement.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ff8b02e",
      "metadata": {
        "id": "1ff8b02e"
      },
      "outputs": [],
      "source": [
        "# TODO: Split"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c93aa09",
      "metadata": {
        "id": "4c93aa09"
      },
      "source": [
        "Voyons voir de quoi a l'air un exemple."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decc39c1",
      "metadata": {
        "id": "decc39c1"
      },
      "outputs": [],
      "source": [
        "data[\"train\"][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f82d86f",
      "metadata": {
        "id": "6f82d86f"
      },
      "source": [
        "On voit que:\n",
        "\n",
        "- Contrairement à PyTorch, qui nous donne des tuples, on a ici un dictionnaire.\n",
        "- L'image est au format PIL, que nous avons vu plus tôt dans le cours\n",
        "- L'annotation est un nombre entier, comme avec PyTorch\n",
        "\n",
        "L'attribut `features` des données nous permet cependant d'accéder à un peu plus de détail sur les données\n",
        "\n",
        "**Exercice: Explorez l'attribut `features` des données d'entraînement et affichez:**\n",
        "\n",
        "- Le nom de la classe 63\n",
        "- Le nombre associé à la classe \"steak\"\n",
        "- Le nombre total de classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a52a3ba3",
      "metadata": {
        "id": "a52a3ba3"
      },
      "outputs": [],
      "source": [
        "test_num = 53\n",
        "test_name = \"steak\"\n",
        "# TODO: Labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nSOmg5i-P_HA",
      "metadata": {
        "id": "nSOmg5i-P_HA"
      },
      "source": [
        "**Exercice: à partir des méthodes ci-dessus, créez un dictionnaire label2idx et un dictionnaire idx2label.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b348a130",
      "metadata": {
        "id": "b348a130"
      },
      "outputs": [],
      "source": [
        "# TODO: Create classes"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9CckNWsZQwnu",
      "metadata": {
        "id": "9CckNWsZQwnu"
      },
      "source": [
        "**Exercice: Affichez un exemple aléatoire tiré des données d'entraînement et affichez le nom de sa classe.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c96222ac",
      "metadata": {
        "id": "c96222ac"
      },
      "outputs": [],
      "source": [
        "# TODO: Display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f900fee4",
      "metadata": {
        "id": "f900fee4"
      },
      "source": [
        "### Préparation des données\n",
        "\n",
        "Comme avec PyTorch, il faut transformer les données de PIL vers des tenseurs.\n",
        "Pour ce faire, Hugginface inclut des classes de type `Preprocessor`.\n",
        "On peut utiliser le pre-processeur d'un modèle pré-entraîné, par exemple ViT entraîné sur les données ImageNet-21K."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8cfffd3",
      "metadata": {
        "id": "e8cfffd3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
        "image_processor"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBAgjoNbRL3v",
      "metadata": {
        "id": "qBAgjoNbRL3v"
      },
      "source": [
        "On peut ensuite utiliser la classe `Preprocessor` pour créer des transformations PyTorch.\n",
        "\n",
        "**Exercice: Utilisez les attributs d'`image_processor` pour créer des transformations PyTorch qui permettront de convertir les données au format attendu par PyTorch. Utilisez Compose pour grouper les transformations suivantes:**\n",
        "\n",
        "- Une modification de la taille à 224 pixels, ou optionnellement un découpage aléatoire.\n",
        "- Une transformation en tenseur\n",
        "- Une normalisation\n",
        "\n",
        "La fonction `transforms` et la méthode `with_transform` permet d'appliquer les transformations aux données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3976a87",
      "metadata": {
        "id": "d3976a87"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import ToTensor, Compose, Resize, Normalize, RandomResizedCrop\n",
        "\n",
        "size = (\n",
        "    image_processor.size[\"shortest_edge\"]\n",
        "    if \"shortest_edge\" in image_processor.size\n",
        "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
        ")\n",
        "# TODO: Ajouter transformations\n",
        "torch_transforms = Compose([\n",
        "])\n",
        "def transforms(examples):\n",
        "    examples[\"pixel_values\"] = [torch_transforms(img) for img in examples[\"image\"]]\n",
        "    del examples[\"image\"]\n",
        "    return examples\n",
        "data = data.with_transform(transforms)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G2gGqs-GSYOy",
      "metadata": {
        "id": "G2gGqs-GSYOy"
      },
      "source": [
        "**Exercice: Affichez un exemple des données pour voir comment elles ont été transformées**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3883379",
      "metadata": {
        "id": "f3883379",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "UJTpAHYfSg6M",
      "metadata": {
        "id": "UJTpAHYfSg6M"
      },
      "source": [
        "On peut aussi définitir un objet [`DataCollator`](https://huggingface.co/docs/transformers/en/main_classes/data_collator). Ceux-ci permettent de convertir les données en sous-ensemble lors de l'etraînement, un peu comme un `DataLoader` dans PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dcfdcf54",
      "metadata": {
        "id": "dcfdcf54",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e567f49",
      "metadata": {
        "id": "8e567f49"
      },
      "source": [
        "### Création du modèle\n",
        "\n",
        "Plus haut, nous avons importé notre modèle via un `pipeline`.\n",
        "Ici, nous allons plutôt importer le modèle directement.\n",
        "Nous utiliserons tout de même un modèle pré-entraîné.\n",
        "\n",
        "**Exercice: Utilisez la méthode `from_pretrained` et le `checkpoint` défini ci-dessus pour créer un modèle. Affichez le modèle ensuite.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a2fc952",
      "metadata": {
        "id": "5a2fc952"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageClassification\n",
        "\n",
        "# TODO: Create model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a51f14f",
      "metadata": {
        "id": "3a51f14f"
      },
      "source": [
        "L'avertissement ci-dessus nous indique que bien que le modèle soit pré-entraîné, son classificateur (la dernière couche) n'est pas entraîné.\n",
        "Il faudra donc ajuster les poids et biais à la tâche qui nous intéresse ici.\n",
        "Par contre, tout le reste du modèle est pré-entraîné.\n",
        "\n",
        "Voyons voir de quoi est fait le modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ff5d379",
      "metadata": {
        "id": "6ff5d379"
      },
      "outputs": [],
      "source": [
        "# TODO: Display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2112eed6",
      "metadata": {
        "id": "2112eed6"
      },
      "source": [
        "Remarquez que la structure générale est la même que celle vue en classe et dans le notebook PyTorch:\n",
        "\n",
        "- Un encodage des images et la position\n",
        "- Un dropout optionnel\n",
        "- Un encodeur, composé ici de 12 blocs ViT, qui eux contiennent:\n",
        "  - Une couche d'attention\n",
        "  - Une connection résiduelle\n",
        "  - Une couche pleinement connectée\n",
        "  - Des normalisations de couche (`LayerNorm`)\n",
        "- Une classificateur permettant de convertir la sortie la sortie du classificateur en score pour chaque catégorie\n",
        "\n",
        "Par défaut, le modèle contient uniquement deux sorties.\n",
        "Il faut l'initialiser avec le bon nombre de classes.\n",
        "On peut également utiliser la conversion entre les numéros de classes et leur nom.\n",
        "\n",
        "**Exercice: Créez un nouveau modèle, mais cette fois avec le bon nombre de classes. Utilisez également les arguments `id2label` et `label2id`.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cafa0030",
      "metadata": {
        "id": "cafa0030"
      },
      "outputs": [],
      "source": [
        "# TODO: Create for num_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35a7b089",
      "metadata": {
        "id": "35a7b089"
      },
      "outputs": [],
      "source": [
        "# TODO: Display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f05884d8",
      "metadata": {
        "id": "f05884d8"
      },
      "source": [
        "### Entraînement\n",
        "\n",
        "Une fois le modèle définit, on peut l'entraîner.\n",
        "Il y aura deux étapes:\n",
        "\n",
        "1. Définir une métrique d'évaluation\n",
        "2. Définir une boucle d'entraînement et l'exécuter.\n",
        "\n",
        "#### Métrique d'évaluation\n",
        "\n",
        "Dans Huggingface, c'est la librairie [`evaluate`](https://huggingface.co/docs/evaluate/index) qui définit les métriques permettant d'évaluer la qualité d'un modèle. Voir ce lien pour une liste des métriques: <https://huggingface.co/evaluate-metric>.\n",
        "\n",
        "Ici, on utilise la métrique de précision (`accuracy`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14c573c4",
      "metadata": {
        "id": "14c573c4"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "accuracy = evaluate.load(\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ca81fa",
      "metadata": {
        "id": "24ca81fa"
      },
      "outputs": [],
      "source": [
        "accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "023fd064",
      "metadata": {
        "id": "023fd064",
        "lines_to_next_cell": 2
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    \"\"\"\n",
        "    Fonction pour évaluer la précision à partir d'un ensemble de prédictions\n",
        "    et de classes de référence.\n",
        "    \"\"\"\n",
        "    preds, labels = eval_pred\n",
        "    preds = np.argmax(preds, axis=1)\n",
        "    return accuracy.compute(predictions=preds, references=labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ce57aa7",
      "metadata": {
        "id": "6ce57aa7"
      },
      "source": [
        "#### Boucle d'entraînement\n",
        "\n",
        "On peut ensuite implémenter une boucle d'entraînement. L'interface est différente de PyTorch, mais on reconnaît la terminologie de plusieurs arguments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "At9v5bXeWQrS",
      "metadata": {
        "id": "At9v5bXeWQrS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27898412",
      "metadata": {
        "id": "27898412"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a34fdb29",
      "metadata": {
        "id": "a34fdb29"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"new_model\",\n",
        "    remove_unused_columns=False,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=4,\n",
        "    report_to=\"none\",\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    logging_steps=10,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ddef392",
      "metadata": {
        "id": "2ddef392"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=data[\"train\"],\n",
        "    eval_dataset=data[\"test\"],\n",
        "    processing_class=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c69c755c",
      "metadata": {
        "id": "c69c755c"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vBmYOK7eZGm4",
      "metadata": {
        "id": "vBmYOK7eZGm4"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(\"new_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NBVIKGoCXzCQ",
      "metadata": {
        "id": "NBVIKGoCXzCQ"
      },
      "source": [
        "### Inférence\n",
        "\n",
        "Une fois le modèle entraîné, on peut l'importer via un pipeline et l'utiliser sur une image. On peut le faire via un pipeline ou en passant directement l'image au modèle. La 2e option est très similaire à l'exemple PyTorch du notebook précédent.\n",
        "\n",
        "**Exercice: Importez des données de validation pour l'ensemble de données utilisé.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fHnBi_tcYd8B",
      "metadata": {
        "id": "fHnBi_tcYd8B"
      },
      "outputs": [],
      "source": [
        "valid_data = load_dataset(\"food101\", split=\"validation[:100]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9jiNINNIYj6Y",
      "metadata": {
        "id": "9jiNINNIYj6Y"
      },
      "outputs": [],
      "source": [
        "valid_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a3379d-ed26-419b-a352-130f75f9d4ff",
      "metadata": {
        "id": "35a3379d-ed26-419b-a352-130f75f9d4ff"
      },
      "source": [
        "**Exercice: Affichez un exemple aléatoire**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i9RqnSeSZRN2",
      "metadata": {
        "id": "i9RqnSeSZRN2"
      },
      "outputs": [],
      "source": [
        "idx = int(rng.integers(valid_data.num_rows))\n",
        "eg = valid_data[idx]\n",
        "img, label = eg[\"image\"], eg[\"label\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q1eLjTfPZjsX",
      "metadata": {
        "id": "Q1eLjTfPZjsX"
      },
      "outputs": [],
      "source": [
        "plt.imshow(img)\n",
        "plt.title(idx2label[label])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f13035c-57c5-4d40-8b30-7068d70c11c7",
      "metadata": {
        "id": "5f13035c-57c5-4d40-8b30-7068d70c11c7"
      },
      "source": [
        "**Exercice: Créez un pipeline avec votre modèle et testez avec un exemple de validation, puis avec un exemple trouvé en ligne.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GFNHKzG0YulW",
      "metadata": {
        "id": "GFNHKzG0YulW"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ibSzyKVfUvgt",
      "metadata": {
        "id": "ibSzyKVfUvgt"
      },
      "source": [
        "## Références\n",
        "\n",
        "- Tutoriel Huggingface duquel celui-ci est inspiré: <https://huggingface.co/docs/transformers/tasks/image_classification>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "phy3051",
      "language": "python",
      "name": "phy3051"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}