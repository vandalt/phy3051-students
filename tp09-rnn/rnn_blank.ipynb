{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "238d2a8e",
   "metadata": {},
   "source": [
    "# Prédiction de séries temporelles avec les réseaux neuronaux récurrents\n",
    "\n",
    "Dans ce TP, nous allons explorer les réseaux neuronaux récurrents sous l'angle de la prédiction de séries temporelles.\n",
    "Le cas de figure que nous allons explorer est un peu différent des séries temporelles que nous avons vues précédemment dans le cours: au lieu de prédire des valeurs $y$ à partir de valeurs $x$, nous allons prédire $y_t$ à partir d'une ou plusieurs valeurs précédentes $y_{t-k}$.\n",
    "\n",
    "## Génération des données\n",
    "\n",
    "Comme premier exemple, nous utiliserons un ensemble de données simulées très simple: une série temporelle non-bruitée, parfaitement périodique et avec un échantillonnage uniforme dans le temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36328086",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "\n",
    "P = 1.0\n",
    "N_cycles = 12.0\n",
    "N_pts = 1000\n",
    "# valeurs de temps, utilisées pour générer y mais pas dans l'analyse\n",
    "t = torch.linspace(0, N_cycles * P, N_pts)\n",
    "y = torch.sin(2 * torch.pi * t / P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd45313",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "plt.plot(t, y)\n",
    "plt.xlabel(\"Temps $t$\")\n",
    "plt.ylabel(\"Valeur $y$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e339511c",
   "metadata": {},
   "source": [
    "## Module RNN avec PyTorch\n",
    "\n",
    "Nous avons vu les équations d'un RNN en classe. Or, PyTorch implémente déjà un RNN avec [nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html). La documentation donne les équations utilisées et une idée conceptuelle de l'implémentation.\n",
    "\n",
    "Nous utiliserons `nn.RNN` dans un réseau sous peu, mais d'abord, créons un modèle composé uniquement d'un bloc RNN, pour en explorer les caractéristiques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd350e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "myrnn = nn.RNN(input_size=1, hidden_size=10, num_layers=1, nonlinearity=\"tanh\", batch_first=True)\n",
    "myrnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4767de4b",
   "metadata": {},
   "source": [
    "Quelques notes sur les arguments ci-dessus:\n",
    "\n",
    "- `input_size` est la dimension d'entrée. Par exemple, chaque point dans notre séquence pourrait être un vecteur en N-dimension. Dans le cas présent, chaque point de la séquence $y$ est un seul chiffre donc `input_size=1`. **Cet argument est requis**\n",
    "- `hidden_size` est la dimension de chaque élément $h_t$ la couche cachée. On choisit une valeur selon la capacité désirée pour notre modèle. **Cet argument est requis**\n",
    "- `num_layers=1` est le nombre de couches dans notre RNN. On pourrait avoir plusieurs couches RNN créées avec le même bloc (1 par défaut).\n",
    "- `nonlinearity=\"tanh\"` est la fonction d'activation à utiliser dans notre bloc RNN\n",
    "- `batch_first=True` (`False` par défault!): Si la dimension des sous-ensembles devrait être avant la dimension de séquence. Voir le prochain paragraphe.\n",
    "\n",
    "Une fois ce bloc créé, on peut lui donner une séquence de points. Tel que mentionné dans la documentation, les entrées accepté par notre RNN sont:\n",
    "\n",
    "- `input`: Un tenseur de format `(L, input_size)` pour une seule séquence de longueur $L$ ou `(batch_size, L, input_size)` pour un sous ensemble de plusieurs séquence. Ici, `batch_size` est en premier car nous avons utilisé `batch_first=True`.\n",
    "- `hx` (optional): Un tenseur contenant l'état initial $h_0$ pour la couche cachée. Initialisé à 0 par défaut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7a4047",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    myrnn(y)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur: '{e}'\")\n",
    "\n",
    "out, hn = myrnn(y.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa2ea33",
   "metadata": {},
   "source": [
    "Comme notre tenseur $y$ est 1D, il faut ajouter la dimension `input_size` de 1, à la position 1 (2e axe) avec `y.unsqueeze(1)`. Examinons les sorties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a2c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"out shape\", out.shape)\n",
    "print(\"hn shape\", hn.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f673ac7",
   "metadata": {},
   "source": [
    "On peut voir que le RNN retourne deux éléments:\n",
    "\n",
    "- `out`: Un tenseur de dimension `(L, hidden_size)` représentant la sortie ($h_t$ de la dernière couche pour tous les temps $t$)\n",
    "- `hn`: Un tenseur de dimension `(num_layers, hidden_size)` représentant le dernier étant caché du réseau (qui serait complètement à droite danns les graphiques vus en classe).\n",
    "\n",
    "Notez que le modèle retourne une sortie pour chaque pas de temps en entrée.\n",
    "Dans notre cas, seul la dernière sortie nous intéresse pour la prédiction.\n",
    "On pourra donc aller indexer `out[..., -1, :]` au besoin.\n",
    "On aurait aussi peut également faire cette modification directement dans le réseau, comme dans la cellule ci-dessous.\n",
    "\n",
    "## Modèle complet RNN avec PyTorch\n",
    "\n",
    "Nous allons maintenant utiliser le bloc RNN décrit ci-dessous dans un réseau un tout petit peu plus complexe afin de faire une prédiction de série temporelle. Notre modèle aura les composantes suivantes:\n",
    "\n",
    "- Un bloc RNN. N'oubliez pas `batch_first=True`!\n",
    "- Un bloc linéaire transformant la dimension cachéé du RNN vers la dimension de sortie désirée\n",
    "\n",
    "**Exercice: Complétez les fonctions `__init__()` et `forward()` du modèle ci-dessous. Créez ensuite un modèle RNN avec 32 dimensions pour h.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ed329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, out_size, rnn_type: str = \"rnn\"):\n",
    "        super().__init__()\n",
    "        # TODO: Couches RNN et linéaire\n",
    "\n",
    "    def forward(self, x, return_seq: bool = False):\n",
    "        # TODO: générer `out` à partir des couches\n",
    "        if not return_seq:\n",
    "            # L'avant dernier axe est celui de la séquence L. On veut seulement le dernier point\n",
    "            out = out[..., -1:, :]\n",
    "        return out\n",
    "\n",
    "# TODO: Créer le modèle avec 32 dimensions intermédiaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c574e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(y.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2cad0-ed9e-4353-9ce6-e5104454c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3cf653-faba-4e57-a68a-e59c63c8897c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(y.unsqueeze(1), return_seq=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed427efa",
   "metadata": {},
   "source": [
    "## Préparation des données\n",
    "\n",
    "La série temporelle `y` n'a pas un format idéal. Afin d'entraîner notre modèle, il faudrait:\n",
    "\n",
    "- Ajouter une dimension `input_dim` pour ne pas avoir à utiliser `.unsqueeze(1)` sans arrêt.\n",
    "- Séparer les données en un ensemble d'entraînement et un ensemble de test.\n",
    "- Structurer les données pour apprendre une prédiction future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac95ccc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = y.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d5a18e",
   "metadata": {},
   "source": [
    "### Données d'entraînement et de test\n",
    "\n",
    "Commençons d'abord par séparer les données d'entraînement et de test.\n",
    "Comme il s'agit d'une tâche de prédiction future (_forecasting_), nous allons séparer les données dans le temps.\n",
    "Le premier 80% constituera l'ensemble d'entraînement, et le reste l'ensemble de test.\n",
    "\n",
    "**Exercice: Séparez `Y` en deux sous-ensembles: un d'entraînement et un de test. Vous pouvez également séparer `t` de la même façon. Ce sera utile pour l'affichage des données.** Nous utiliserons une fonction pour pouvoir réappliquer cette logique au besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db6ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Séparation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af31f38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, Y, \"k\", alpha=0.2)\n",
    "plt.plot(t_train, Y_train, label=\"Training set\")\n",
    "plt.plot(t_test, Y_test, label=\"Test set\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5308329d",
   "metadata": {},
   "source": [
    "### Structure des données pour l'apprentissage\n",
    "\n",
    "Comme la tâche qui nous intéresse est une prédiction future, il faut structurer nos données un peu différemment d'à l'habitude. Il faut d'abord réfléchir à ce qui constitue notre entrée (_features_, l'équivalent de l'image dans un problème de classification) et notre sortie ou _cible_ (l'équivalent de la catégorie dans une classification).\n",
    "\n",
    "Le cas de prédiction le plus simple à considérer est le suivant: nous utiliserons une _fenêtre_ de $N$ points passés pour prédire le prochain point. Pour entraîner un modèle à résoudre ce problème, il nous faut restructurer les données de test et d'entraînement. Utilisons une fenêtre de 40 points, par exemple. On commence par prendre les 40 premiers points de notre ensemble d'entraînement comme entrée (_feature_). La cible correspondante est le point suivant, donc le point 41. Ceci constitue notre premier exemple d'entraînement. Le 2e exemple aura comme entrée les poitns 2 à 41 et comme sortie le point 42, et ainsi de suite.\n",
    "\n",
    "**Note: Le choix de 40 points est arbitraire ici. D'autres valeurs donneraient des caractéristiques différentes au modèle. En général plus la fenêtre dont on dispose est longue, plus on peut s'attendre à ce que le pouvoir prédictif soit robuste.**\n",
    "\n",
    "On peut mettre la logique décrite ci-dessus dans une fonction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660c34a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windowed_data(data: Tensor, window_size: int, target_size: int = 1) -> Tuple[Tensor, Tensor]:\n",
    "    N_samples = len(data) - window_size\n",
    "    assert N_samples > 0, \"Negative numer of samples: less points than window size\"\n",
    "    features, targets = [], []\n",
    "    for i in range(N_samples):\n",
    "        features.append(data[i:i+window_size])\n",
    "        targets.append(data[i+window_size:i+window_size+target_size])\n",
    "    return torch.stack(features), torch.stack(targets)\n",
    "\n",
    "window_size = 40\n",
    "train_features, train_targets = get_windowed_data(Y_train, window_size)\n",
    "test_features, test_targets = get_windowed_data(Y_test, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a09d682",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Format des features d'entraînement:\", train_features.shape)\n",
    "print(\"Format des targets d'entraînement:\", train_targets.shape)\n",
    "print(\"Format des features de test:\", test_features.shape)\n",
    "print(\"Format des targets de test:\", test_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f3d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t_train[:window_size], train_features[0], \".\", label=\"Entrée\")\n",
    "plt.plot(t_train[window_size], train_targets[0], \".\", label=\"Cible\")\n",
    "plt.title(\"Premier exemple d'entraînement\")\n",
    "plt.xlabel(\"t\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b62263",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(train_features)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f893e11",
   "metadata": {},
   "source": [
    "**Exercice: Copiez le graphique ci-dessus et affichez la prédiction initiale du modèle pour cet exemple.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603848a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf5265",
   "metadata": {},
   "source": [
    "En exploitant une dimension de sous-ensemble, on peut également générer la prédiction du modèle pour toutes les fenêtre en même temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20e2f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = model(train_features)[:, -1, :]\n",
    "test_pred = model(test_features)[:, -1, :]\n",
    "\n",
    "plt.plot(t, Y, \"k\", alpha=0.2)\n",
    "plt.plot(t_train, Y_train, label=\"Training set\")\n",
    "plt.plot(t_test, Y_test, label=\"Test set\")\n",
    "plt.plot(t_train[window_size:], train_pred.detach(), \"--\", label=\"Training prediction\")\n",
    "plt.plot(t_test[window_size:], test_pred.detach(), \"--\", label=\"Test prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662015a",
   "metadata": {},
   "source": [
    "Remarquez que comme nous utilisons la fenêtre pour générer nos prédiction, les $N$ premiers points n'ont pas de prédiction, tant dans les données d'entraînement que dans les données test.\n",
    "\n",
    "### Dataset PyTorch\n",
    "\n",
    "Les données ci-dessus pourraient être utilisées directement pour entraîner le réseau, mais nous n'avons qu'un seul ensemble.\n",
    "Il serait préférable de partitioner les données en sous-ensembles.\n",
    "\n",
    "Bonne nouvelle: on peut créer un `Dataset` PyTorch à partir de n'importe quelle paire de tenseurs! Il suffit d'utiliser `TensorDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54878bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "train_set = TensorDataset(train_features, train_targets)\n",
    "test_set = TensorDataset(test_features, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39cb00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_set, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e033fb2a",
   "metadata": {},
   "source": [
    "## Entraînement du modèle\n",
    "\n",
    "**Exercice: Définissez une boucle d'entraînement pour le modèle.**\n",
    "\n",
    "- Définissez un modèle (pour que chaque exécution de la cellule réinitialise les poids)\n",
    "- Utilisez une fonction objectif MSE\n",
    "- Utilisez un optimiseur Adam\n",
    "- Emmagasinez la valeur de la fonction objectif pour les données test et d'entraînement à chaque époque.\n",
    "- Suggestion: afin de voir le modèle s'améliorer en temps réel, calculez la prédiction sur toutes les données d'entraînement et de test à chaque époque (en utilisant `train_features` et `test_features` directement) et affichez la sur un graphique similaire à celui montré ci-dessus pour les paramètres initiaux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f894d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e6a0ba-bf63-41d2-aba6-86c33f507c92",
   "metadata": {},
   "source": [
    "**Exercice: Affichez l'évolution de la fonction objectif**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c67e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evolution de la fonction objectif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80ae6bb",
   "metadata": {},
   "source": [
    "## Prédiction finale\n",
    "\n",
    "### Prédiction d'un point\n",
    "\n",
    "**Exercice: Affichez la prédiction finale pour les données d'entraînement et de test**\n",
    "\n",
    "Avec la configuration initiale, la prédiction devrait être presque parfaite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730af7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6689f91c",
   "metadata": {},
   "source": [
    "### Prédiction auto-régressive\n",
    "\n",
    "Une autre façon d'utiliser un modèle comme celui-ci est de manière auto-régressive: on utilise la dernière prédiction du modèle pour faire plus de pas dans le futur.\n",
    "\n",
    "Voici un petit exemple. Comme on peut voir, le résultat n'est pas excellent. Ce n'est pas trop surprenant: nous n'avons pas explicitement entraîné notre modèle pour accomplir cette tâche. Par contre, certains modèles (ex.: LSTM) performent mieux que d'autres. La taille de la fenêtre utilisée dans l'entraînement aura également un impact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8b6f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_Y = torch.cat([Y_train, torch.empty(200, 1)])\n",
    "with torch.no_grad():\n",
    "    for i in range(N_train, len(new_Y)):\n",
    "        new_pred = model(new_Y[i-window_size:i])\n",
    "        new_Y[i] = new_pred[..., -1:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd42fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y[:N_train])\n",
    "plt.plot(new_Y, \"--\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792b7ecf-3ed9-4a67-9735-a135961d67ad",
   "metadata": {},
   "source": [
    "## Exercice: Exploration des hyperparamètres\n",
    "\n",
    "1. Testez l'impact de la fenêtre d'entraînement. Sur la qualité des prédictions Vous pouvez tester une fenêtre de 1, de 10, de 80, etc.\n",
    "2. Testez l'impact de remplacer le RNN par une autre architecture vue en classe, telle que le LSTM ou le GRU. Quel est l'impact sur l'entraînement? Et sur la prédiction?\n",
    "3. Variez le nombre de dimensions pour la couche cachée du RNN. Quel est l'impact sur la prédiction?\n",
    "\n",
    "Ce ne sont que quelques suggetions. N'hésitez pas à explorer d'autres modifications!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2e87c",
   "metadata": {},
   "source": [
    "## Exercice: Données de CO2 Mauna Loa\n",
    "\n",
    "Un exemple souvent utilisé pour les extrapolations est celui de l'évolution du CO2 mesuré sur le Mauna Loa.\n",
    "La libraire `statsmodels` donne accès à ces données.\n",
    "\n",
    "**Exercice: Utilisez le RNN ci-dessus pour prédire les données du Mauna Loa. Comment se compare la qualité des préductions avec l'exemple simple du sinusoide?**\n",
    "\n",
    "Vous pouvez simplement copier la cellule ci-dessous en haut du notebook et commenter la cellule où le sinusoide est généré: tout devrait fonctionner!\n",
    "\n",
    "Vous devrez fort probablement changer le nombre d'époques dans l'entraînement.\n",
    "\n",
    "Aussi, une simple transformation des données aide à accélérer l'entraînement. Pouvez-vous identifier laquelle?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0582e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.datasets import co2\n",
    "\n",
    "y = torch.from_numpy(co2.load_pandas().data.values.ravel().astype(\"float32\"))\n",
    "y = y[~torch.isnan(y)]\n",
    "t = torch.arange(len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596ba512",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(t, y)\n",
    "plt.ylabel(\"CO2 [ppm]\")\n",
    "plt.xlabel(\"Indice i\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86a2084-e915-413c-ac10-2e18ef81c838",
   "metadata": {},
   "source": [
    "## Exercice: Exploration plus avancée\n",
    "\n",
    "Voici quelques exercices bonus à essayer s'il vous reste du temps. Le tutoriel TensorFlow ci-dessous explore notamment ces questions:\n",
    "\n",
    "- Comment pourriez-vous modifier le réseau pour prédire plus d'un pas dans le temps?\n",
    "- Pourriez-vous créer un CNN pour obtenir des résultats similaires?\n",
    "- Comment pourriez-vous inclure le temps comme entrée dans votre réseau? Ceci permettrait de tenir compte de pas de temps irréguliers!\n",
    "- Testez un GP sur les données de Mauna Loa et comparez sa prédiction avec celle de votre réseau."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc4fa6",
   "metadata": {},
   "source": [
    "## Références\n",
    "\n",
    "Voici quelques références intéressantes sur le sujet.\n",
    "\n",
    "- [LSTM for Time Series Prediction in PyTorch (Machine Learning Mastery)](https://machinelearningmastery.com/lstm-for-time-series-prediction-in-pytorch/): Tutoriel similaire à la première partie du TP, également en PyTorch. Ce site web a plusieurs tutoriels relativement cours et bien construits si vous voulez explorer certains sujets du cours plus en détail.\n",
    "- [Tutoriel TensorFlow sur la prédiction météo](https://www.tensorflow.org/tutorials/structured_data/time_series#recurrent_neural_network): plus avancé que ce TP, mais utilise plusieurs idées similaires. Très intéressant si vous souhaitez approfondir les notions couvertes ici!\n",
    "- [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850): article sur la génération de séquences avec les RNNs"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
