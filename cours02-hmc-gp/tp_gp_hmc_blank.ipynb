{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e264d9d9",
   "metadata": {},
   "source": [
    "# Modélisation de bruit corrélé autour d'un modèle physique avec les processus gaussiens\n",
    "\n",
    "Au dernier cours, nous avons vu comment implémenter une version simple des processus gaussiens.\n",
    "Tel que vu en classe, un processus gaussien permet de modéliser le bruit corrélé autour d'un modèle paramétrique (souvent appelé \"mean function\" dans la littérature).\n",
    "Nous allons effectuer une telle analyse avec un modèle physique assez similaire à celui utilisé dans le devoir 1.\n",
    "\n",
    "Pour couvrir du même coup les GPs et le HMC, nous allons utiliser la librairie `tinygp`, qui utilise `jax` afin de facilement calculer le gradient des modèles GPs.\n",
    "J'ai ajouté un notebook `tinygp.ipynb` dans le TP sur les processus gaussiens, qui pourra servir de référence.\n",
    "\n",
    "Pour l'échantillonnage HMC, nous allons utiliser NumPyro, une librairie de programmation probabiliste (PPL).\n",
    "Elle est aussi basée sur JAX.\n",
    "\n",
    "## Définition du modèle physique\n",
    "\n",
    "Supposons qu'on cherche l'amplitude, la largeur et la position d'une raie d'émission. On additionne aussi une constante pour modéliser le niveau du continuum. Notre modèle sera donc\n",
    "\n",
    "$$\n",
    "m(x) = b + a \\exp{\\left(-\\frac{(x-\\ell)^2}{2 w^2}\\right)}\n",
    "$$\n",
    "\n",
    "Pour que le modèle soit compatible avec `tinygp` et NumPyro, nous allons le créer avec `jax`.\n",
    "\n",
    "**Implémentez le modèle ci-dessus avec JAX (`jnp`). Utilisez un dictionnaire de paramètres comme premier argument et une valeur de $x$ comme deuxième argument.**\n",
    "\n",
    "_Note: C'est un hasard qu'on utilise une **gaussienne** comme modèle paramétrique avec notre processus **gaussien**. La fonction paramétrique (\"mean function\") aurait pu être une droite, un sinus, un transit, etc., selon les données qu'on observe._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45e8998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Précision machine \"double\" avec Jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "def gaussian_model(params: dict, x: float):\n",
    "    \"\"\"\n",
    "    Modèle d'une gaussienne et d'un terme constant.\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    params : dict\n",
    "        Dictionnaire de paramètres avec des éléments \"loc\", \"log_width\", \"a\" et \"b\".\n",
    "    X : float\n",
    "        Valeur(s) X pour laquelle on calcule la fonction.\n",
    "    \"\"\"\n",
    "    # Le modèle de gaussienne\n",
    "    mod = params[\"b\"] + params[\"a\"] * jnp.exp(-0.5 * jnp.square((x - params[\"loc\"]) / jnp.exp(params[\"log_width\"])))\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8443e284",
   "metadata": {},
   "source": [
    "**Testez votre modèle avec des valeurs quelconques pour les paramètres.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2119f81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Définir dictionnaire mean_params\n",
    "X_grid = np.linspace(0, 10, num=200)\n",
    "mean_params = {\n",
    "    \"a\": 1.0,\n",
    "    \"b\": 0.0,\n",
    "    \"log_width\": jnp.log(1.0),\n",
    "    \"loc\": 5.0,\n",
    "}\n",
    "\n",
    "ymodel = gaussian_model(mean_params, X_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d0a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X_grid, ymodel)\n",
    "plt.title(\"Modèle gaussien paramétrique\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3aea2b",
   "metadata": {},
   "source": [
    "## Données simulées\n",
    "On peut générer des données simulées à partir de la gaussienne, mais aussi d'un signal additionnel \"inconnu\" qu'on pourra modéliser avec un GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7cc4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "random = np.random.default_rng(135)\n",
    "X = np.sort(random.uniform(0, 10, 50))\n",
    "X_test = np.linspace(X.min() - 2.0, X.max() + 2.0, num=200)\n",
    "y = gaussian_model(mean_params, X)\n",
    "true_y = gaussian_model(mean_params, X_test)\n",
    "y += 0.1 * np.sin(2 * np.pi * (X - 5) / 10.0)\n",
    "true_y += 0.1 * np.sin(2 * np.pi * (X_test - 5) / 10.0)\n",
    "y += 0.03 * random.normal(size=len(X))\n",
    "yerr = np.abs(0.003 * random.normal(size=len(X)) + 0.01)\n",
    "\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.title(\"Données simulées\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f16db0d",
   "metadata": {},
   "source": [
    "## Modèle GP\n",
    "\n",
    "Il est maintenant temps de définir le modèle GP que nous allons utiliser pour les données.\n",
    "La fonction \"moyenne\" reste la même (une gaussienne), mais on ajoute un GP à notre modèle pour s'occuper du signal additionnel.\n",
    "On utilise un kernel Matérn-5/2. En partique, on en testerait probablement quelques uns pour vérifier que le choix est optimal.\n",
    "\n",
    "On construit le GP dans une fonction afin de facilement mettre à jour les paramètres à chaque itération d'un optimiseur ou d'un MCMC.\n",
    "\n",
    "**Définissez une fonction `build_gp` qui retourne un objet `tinygp.GaussianProcess`.**\n",
    "**Référez vous aux annotations pour les détails concernant les paramètres.**\n",
    "**Assurez-vous de bien convertir les paramètres donnés en échelle log via `jnp.exp`.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7519ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygp import kernels, GaussianProcess\n",
    "\n",
    "def build_gp(\n",
    "        params: dict, X: Union[np.ndarray, jax.Array], yerr: Union[np.ndarray, float, jax.Array] = 0.0\n",
    ") -> GaussianProcess:\n",
    "    \"\"\"\n",
    "    Création d'un modèle GP\n",
    "\n",
    "    Paramètres\n",
    "    ----------\n",
    "    params : dict\n",
    "        Paramètres du modèle dans un dictionnaire. Devrait contenir les paramètres log_gp_amp, log_gp_scale et log_gp_diag, en plus des paramètres pour gaussian_model()\n",
    "    X : Union[np.ndarray, jax.Array]\n",
    "        Coordonnées d'entrée X du GP pour le calcul de la vraisemblance\n",
    "    yerr: Union[np.ndarray, float, jax.Array]\n",
    "        Erreur sur les mesures. Le carré des erreurs est ajouté à la diagonale.\n",
    "        Pour générer des échantillons du GP. ce paramètre devrait être 0.0\n",
    "    \"\"\"\n",
    "    kernel = jnp.exp(params[\"log_gp_amp\"]) * kernels.Matern52(\n",
    "        jnp.exp(params[\"log_gp_scale\"])\n",
    "    )\n",
    "    \n",
    "    return GaussianProcess(kernel, X, diag=yerr**2 + jnp.exp(params[\"log_gp_diag\"]), mean=lambda x: gaussian_model(params, x))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da04ce",
   "metadata": {},
   "source": [
    "### Distribution _prior_ du GP\n",
    "\n",
    "On peut d'abord vérifier de quoi on l'air les échantillons individuels du GP _à priori_, avant d'avoir montré des données au modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b71db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "params_ini = {\n",
    "    \"log_gp_amp\": np.log(0.1),\n",
    "    \"log_gp_scale\": np.log(3),\n",
    "    # Petite valeur pour afficher des prédictions qui ne tiennent pas compte de données.\n",
    "    # Si on utilisait une grande valeur, il y aurait un bruit \"blanc\" (gaussien indépendant) sur chacun des points dans l'échantillon prior.\n",
    "    \"log_gp_diag\": np.log(1e-8),\n",
    "    \"a\": 0.2,\n",
    "    \"b\": 0.1,\n",
    "    \"loc\": 4.5,\n",
    "    \"log_width\": np.log(1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605d3ce6",
   "metadata": {},
   "source": [
    "**Créez un objet `gp` avec `build_gp` et affichez 10 échantillons du prior pour `X_test`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336490a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(params_ini, X_test)\n",
    "\n",
    "gp_prior_samples = gp.sample(jax.random.key(8), shape=(10,))\n",
    "print(gp_prior_samples.shape)\n",
    "\n",
    "plt.plot(X_test, gp_prior_samples.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf877ed",
   "metadata": {},
   "source": [
    "On peut également vérifier de quoi ont l'air la moyenne et la variance du GP.\n",
    "\n",
    "**Copiez le graphique ci-dessus, mais ajoutez la moyenne et l'intervalle $1~\\sigma$ du GP.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083fd391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: plot with GP mean and sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845bf739",
   "metadata": {},
   "source": [
    "On voit que le prior du GP est très incertain autour de la moyenne. En montrant des données au GP, la situation devrait s'améliorer un peu."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba33dd2",
   "metadata": {},
   "source": [
    "### GP conditionné (_posterior_)\n",
    "\n",
    "Maintenant, _sans modifier les hyperparamètres_, on peut conditionner le GP sur les données. Mêmes si les hyperaparamètres ne sont pas optimaux, on devrait au moins obtenir une prédiction qui ne passe pas trop loin des données.\n",
    "\n",
    "**Utilisez `build_gp` pour générer un GP sur les données `X`. Tenez aussi compte de l'erreur `yerr` sur les observations.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e26b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajout d'un terme de diagonale, qu'on avait initialisé à 0 pour tirer des échantillons\n",
    "params_ini[\"log_gp_diag\"] = np.log(0.001)\n",
    "gp = build_gp(params_ini, X, yerr=yerr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9f0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_posterior_samples = cond_gp.sample(jax.random.key(8), shape=(10,))\n",
    "\n",
    "plt.plot(X_test, gp_posterior_samples.T, alpha=0.6, linestyle=\"--\")\n",
    "plt.plot(X_test, cond_gp.mean)\n",
    "plt.fill_between(X_test, cond_gp.mean - np.sqrt(cond_gp.variance), cond_gp.mean + np.sqrt(cond_gp.variance), alpha=0.3)\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.title(\"Données simulées\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57cf51d",
   "metadata": {},
   "source": [
    "**Conditionnez ensuite le GP sur les observations `y`. On veut générer la prédiction sur `X_test`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb906b3",
   "metadata": {},
   "source": [
    "Déjà beaucoup mieux. Le modèle passe maintenant plus près des données. L'incertitude est également contrainte.\n",
    "Par contre, on peut tout de même noter un petite écart dans le pic de la gaussienne. Et les échantillons oscillent un peu rapidement, faisant perdre de la capacité d'extrapolation.\n",
    "De meilleurs hyperparamètres devraient améliorer la situation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0802730e",
   "metadata": {},
   "source": [
    "## Optimisation avec Jaxopt\n",
    "\n",
    "Il existe plusieurs librairies d'optimisation avec Jax. Ici, on utilise Jaxopt.\n",
    "Comme Jax calcule automatiquement les gradients, on peut tirer avantage des algorithmes d'optimisation avec gradient.\n",
    "\n",
    "On doit d'abord définir une \"loss function\", que l'on souhaite minimiser. Il s'agit ici de la vraisemblance négative (toujours en log).\n",
    "\n",
    "**Utilisez jaxopt pour optimiser le modèle**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a047e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "# TODO: Définir une fonction \"loss\" et optimiser avevc jaxopt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6734d56",
   "metadata": {},
   "source": [
    "On peut ensuite chercher le minimum avec une interface similaire à scipy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd50444",
   "metadata": {},
   "source": [
    "Regardons maintenant de quoi a l'air notre modèle conditionné sur les données, avec les nouveaux hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ddc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params, X, yerr)\n",
    "_, cond = gp.condition(y, X_test)\n",
    "\n",
    "mu = cond.loc\n",
    "std = np.sqrt(cond.variance)\n",
    "samp = cond.sample(jax.random.PRNGKey(33), shape=(5,))\n",
    "\n",
    "plt.plot(X_test, mu, label=\"Modèle\")\n",
    "plt.plot(X_test, true_y, \"k\", lw=1.5, alpha=0.3, label=\"Signal simulé\")\n",
    "plt.fill_between(X_test, mu + std, mu - std, color=\"C0\", alpha=0.3)\n",
    "plt.plot(X_test, samp.T)\n",
    "plt.errorbar(X, y, yerr, fmt=\"k.\", capsize=2)\n",
    "plt.xlim(X_test.min(), X_test.max())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72241ff9",
   "metadata": {},
   "source": [
    "Pas si mal! Par contre, difficile de voir ce qui vient du GP et ce qui vient de notre modèle gaussiens... Pour ce faire, on peut séparer les contributions de chaque terme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bef5d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params, X, yerr)\n",
    "_, cond = gp.condition(y, X_grid, include_mean=False)\n",
    "\n",
    "mu = cond.loc + soln.params[\"b\"]\n",
    "std = np.sqrt(cond.variance)\n",
    "\n",
    "plt.errorbar(X, y, yerr=yerr, fmt=\"k.\", label=\"Données\", capsize=2)\n",
    "plt.plot(X_grid, mu, label=\"Modèle GP\")\n",
    "plt.fill_between(X_grid, mu + std, mu - std, color=\"C0\", alpha=0.3)\n",
    "plt.plot(X_grid, jax.vmap(gp.mean_function)(X_grid), label=\"Model moyen (gaussienne)\")\n",
    "\n",
    "plt.xlim(X_grid.min(), X_grid.max())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3b15df",
   "metadata": {},
   "source": [
    "## HMC avec Numpyro\n",
    "\n",
    "Comme JAX nous donne automatiquement les gradients, il existe plusieurs librairies pour tirer avantage du HMC avec JAX.\n",
    "L'une des plus communes est NumPyro, qui implémente plusieurs distribution et simplifie beaucoup la définition d'un modèle probabiliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffcc7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836b3050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliser 2 CPUs pour le MCMC (1 par chaîne)\n",
    "numpyro.set_host_device_count(2)\n",
    "\n",
    "def numpyro_model(X, yerr, y=None):\n",
    "    # Definition de nos paramètres (priors).\n",
    "    # Comme notre modèle utilise un dictionnaire, on place les distribution dans un dictionnaire\n",
    "    params = dict(\n",
    "        log_gp_amp = numpyro.sample(\"log_gp_amp\", dist.Uniform(jnp.log(1e-4), jnp.log(1))),\n",
    "        log_gp_scale = numpyro.sample(\"log_gp_scale\", dist.Uniform(np.log(1e-4), np.log(1e2))),\n",
    "        log_gp_diag = numpyro.sample(\"log_gp_diag\", dist.Uniform(np.log(1e-4), np.log(1))),\n",
    "        a = numpyro.sample(\"a\", dist.LogUniform(1e-4, 10)),\n",
    "        b = numpyro.sample(\"b\", dist.Uniform(-0.1, 0.5)),\n",
    "        loc = numpyro.sample(\"loc\", dist.Uniform(0, 10)),\n",
    "        log_width = numpyro.sample(\"log_width\", dist.Uniform(np.log(1e-1), np.log(10.0))),\n",
    "        \n",
    "    )\n",
    "\n",
    "    # Définition du modèle GP en incluant les priors\n",
    "    gp = build_gp(params, X, yerr)\n",
    "\n",
    "    # Ceci fait office de likelihood. Tinygp implémente directement une distribution numpyro\n",
    "    # En utilisant \"obs=y\", on échantillonne selon les observations.\n",
    "    #numpyro.sample(\"obs\", dist.Normal(ymodel, yerr), obs=y)\n",
    "    numpyro.sample(\"gp\", gp.numpyro_dist(), obs=y)\n",
    "\n",
    "    # Il est intéressant de regarder la prédiction du GP à travers le sampling\n",
    "    if y is not None:\n",
    "        numpyro.deterministic(\"pred\", gp.condition(y, X_test).gp.loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a16508e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numpyro utilise un \"kernel\" pour le MCMC, i.e. pour définir les propositions, un peu comme les \"moves\" emcee.\n",
    "# Ici on utilise le No U-Turn Sampling, la version la plus commune du HMC.\n",
    "# On lance le MCMC avec 1000 warm-up (burn in) et 1000 échantillons. On utilise 2 chaines (2 walkers dans emcee). Par contre ici les chaines sont indépendantes.\n",
    "# TODO: MCMC definition\n",
    "nuts_kernel = NUTS(model=numpyro_model, dense_mass=True, target_accept_prob=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5889388",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcmc = MCMC(nuts_kernel, num_warmup=1000, num_samples=1000, num_chains=2, progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda6a892",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avec Jax, il faut générer explicitement les nombres aléatoires\n",
    "# TODO: Run MCMC\n",
    "mcmc.run(jax.random.key(344), X, yerr, y=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eae820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va ensuite chercher les échantillons\n",
    "samples = mcmc.get_samples()\n",
    "pred = samples[\"pred\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7cc5668",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e930d3",
   "metadata": {},
   "source": [
    "Le MCMC est terminé. On a tout ce dont on a besoin pour estimer l'incertitude sur le GP.\n",
    "\n",
    "### Visualisation\n",
    "\n",
    "La librairie arviz permet de visualiser facilement un résumé des résultats et les statistiques associées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b4f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecc0b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata = az.from_numpyro(mcmc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc62ddf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "idata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad7ad0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(idata, var_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cd562e",
   "metadata": {},
   "source": [
    "Le \"trace plot\" montre les chaînes et la distribution poru chaque paramètre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a7c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[v for v in idata.posterior.data_vars if v != \"pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac78b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names=[v for v in idata.posterior.data_vars if v != \"pred\"]\n",
    "mean_params = {\n",
    "    \"a\": 0.3,\n",
    "    \"b\": 0.1,\n",
    "    \"loc\": 5.0,\n",
    "    \"log_width\": np.log(0.5),\n",
    "}\n",
    "true_params = {name: mean_params.get(name) for name in var_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81bd4bf",
   "metadata": {},
   "source": [
    "Corner est compatible avec numpyro et arviz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a383a199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d49afa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(idata, var_names=[\"a\", \"b\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33fa6d9",
   "metadata": {},
   "source": [
    "Arviz donne aussi l'autocorrélation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdfabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_autocorr(\n",
    "    data, var_names=[v for v in data.posterior.data_vars if v != \"pred\"]\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2932a30b",
   "metadata": {},
   "source": [
    "On peut utiliser les quantiles du MCMC pour voir l'impact de l'incertitude sur les hyperparamètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17439ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot MCMC samples/quantiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
