{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f5f689",
   "metadata": {},
   "source": [
    "# Apprentissage non supervisé\n",
    "\n",
    "Dans ce TP, nous allons couvrir certaines méthodes d'apprentissage non supervisé.\n",
    "Nous ne couvrirons pas les modèles génératifs profonds basés sur les réseaux neuronaux: ils seront couverts dans les prochaines semaines. Ici, on se concentre sur des méthodes plus classiques:\n",
    "\n",
    "- Les modèles de mélange\n",
    "- L'analyse de composantes principales\n",
    "- La méthode des $k$-moyennes\n",
    "\n",
    "Nous utiliserons la librairie [`scikit-learn`](https://scikit-learn.org/stable/index.html) (`sklearn`), qui implémente plusieurs algorithmes d'apprentissage automatique, tant [supervisés](https://scikit-learn.org/stable/supervised_learning.html) que [non supervisés](https://scikit-learn.org/stable/unsupervised_learning.html).\n",
    "Vous pouvez l'installer avec `python -m pip install scikit-learn`.\n",
    "\n",
    "## Modèle de mélange (_mixture model_)\n",
    "\n",
    "Les modèles de mélange, ou _mixture models_ en anglais, représente les données comme des points aléatoires tirés d'un nombre fini de distributions de probabilité.\n",
    "La version la plus commune est le modèle de mélange gaussien (_Gaussian mixture model_ ou GMM), qui utilise des distributions normales.\n",
    "\n",
    "### Données simulées\n",
    "\n",
    "Commençons par un exemple simple en 2D.\n",
    "On peut générer des données gaussiennes avec plusieurs pics et écart-types.\n",
    "`scikit-learn` inclut la fonction `make_blobs` pour générer ce genre de données.\n",
    "\n",
    "Nous avons trois types de distributions implémentées ci-dessous:\n",
    "\n",
    "- `5-random`: 5 pics dont le centre est aléatoire et l'écart-type fixe.\n",
    "- `5-preset`: 5 pics dont le centre est pré-déterminé\n",
    "- `2-diag`: 2 pics, dont l'un a une covariance relativement élevée.\n",
    "- `2-diag`: 2 pics avec la même covariance\n",
    "\n",
    "N'hésitez pas à les tester tous les trois en modifiant `gmm_dist` ci-dessous.\n",
    "Vous pouvez également changer les paramètres (position, écart type, covariance) pour voir l'impact sur la qualité du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83aed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm, colors\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.mixture import GaussianMixture\n",
    "import numpy as np\n",
    "\n",
    "plt.style.use('tableau-colorblind10')\n",
    "\n",
    "N_samples = 100\n",
    "\n",
    "def generate_data(N_samples, dist: str):\n",
    "    if dist == \"5-random\":\n",
    "        N = 5\n",
    "        X, y, centers = make_blobs(n_samples=N_samples, centers=N, cluster_std=1.0, return_centers=True)\n",
    "    elif dist == \"5-preset\":\n",
    "        centers = np.array([[0, 0], [1, 1], [1, -1], [-1, 1], [-1, -1]])\n",
    "        N = len(centers)\n",
    "        X, y = make_blobs(n_samples=N_samples, centers=centers, cluster_std=0.1)\n",
    "    elif dist == \"1-diag-1-circle\":\n",
    "        centers = np.array([[0, 0], [20, 20]])\n",
    "        N = len(centers)\n",
    "        A = np.array([[0.0, -0.7], [3.5, 0.7]])  # Cov = A.A^T\n",
    "        X, y = make_blobs(n_samples=N_samples, centers=centers, cluster_std=1.0)\n",
    "        X[y == 0] = X[y == 0] @ A\n",
    "    elif dist == \"2-diag\":\n",
    "        centers = np.array([[0, 0], [1, 11]])\n",
    "        N = len(centers)\n",
    "        A = np.array([[1.0, -9.0], [0.1, 2.0]])  # Cov = A.A^T\n",
    "        X, y = make_blobs(n_samples=N_samples, centers=centers, cluster_std=1.0)\n",
    "        X = X @ A\n",
    "    elif dist == \"1-diag\":\n",
    "        centers = np.array([[0, 0]])\n",
    "        N = len(centers)\n",
    "        A = np.array([[1.0, -9.0], [0.1, 2.0]])  # Cov = A.A^T\n",
    "        X, y = make_blobs(n_samples=N_samples, centers=centers, cluster_std=1.0)\n",
    "        X = X @ A\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown distribution {dist}\")\n",
    "    return X, y, centers, N\n",
    "\n",
    "gmm_dist = \"1-diag-1-circle\"\n",
    "X, y, centers, N = generate_data(N_samples, gmm_dist)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, alpha=1.0, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(f\"Données pour la distribution '{gmm_dist}'\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb2f6f7",
   "metadata": {},
   "source": [
    "Affichons les différents groupes clairement, comme on les connait dans ce cas-ci."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ccda5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Optional\n",
    "\n",
    "\n",
    "def discrete_cmap(N: int, base_cmap: Optional[Union[str, colors.Colormap]] = None):\n",
    "    \"\"\"\n",
    "    Création d'une colormap discrète avec N catégories.\n",
    "\n",
    "    - N: Nombre de points\n",
    "    - base_cmap: Colormap qui sera divisée\n",
    "\n",
    "    Ref: https://gist.github.com/jakevdp/91077b0cae40f8f8244a\n",
    "    \"\"\"\n",
    "    if base_cmap is None:\n",
    "        base = plt.get_cmap(base_cmap, N)\n",
    "        return base\n",
    "    else:\n",
    "        base = plt.cm.get_cmap(base_cmap)\n",
    "    color_list = base(np.linspace(0, 1, N))\n",
    "    cmap_name = base.name + str(N)\n",
    "    return base.from_list(cmap_name, color_list, N)\n",
    "\n",
    "\n",
    "cmap = discrete_cmap(N)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, alpha=1.0, edgecolor=\"k\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Données affichées avec leur vrai groupe\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e15db6",
   "metadata": {},
   "source": [
    "Dans un scénario d'analyse réaliste, `y` serait possiblement inconnu et notre but serait de séparer les données en `N` distributions.\n",
    "On peut donc afficher les données sans leur groupe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce7770f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], color=\"C0\", alpha=0.6, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Données simulées\")\n",
    "plt.axis(\"square\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2642a1",
   "metadata": {},
   "source": [
    "### Modèle avec scikit-learn\n",
    "\n",
    "L'interface scikit-learn est à peu près la même pour tous les modèles:\n",
    "\n",
    "1. On crée et configure notre modèle en créant un objet, ici une `GaussianMixture`\n",
    "2. On entraîne le modèle sur les données avec `model.fit(X)` (ici `GaussianMixture.fit`)\n",
    "3. On utilise le modèle entraîner pour extraire de l'information sur le problème, générer des exemples, etc.\n",
    "\n",
    "Commençons par créer le modèle et l'ajuster aux données. On doit spécifier le nombre de distributions (`n_components`) et le type de covariance:\n",
    "\n",
    "- `spherical` pour ne permettre aucune covariance\n",
    "- `tied` pour que tous les modes aient la même covariance\n",
    "- `diag` pour que chaque distribution ait sa matrice de covariance diagonale\n",
    "- `full` pour que chaque distribution ait sa matrice de covariance générale\n",
    "\n",
    "\n",
    "Ici, on utilise également un argument `means_init`. Ceci facilite l'optimisation, mais permet surtout de s'assurer que le premier pic dans nos `y` ci-dessus sera le premier pic dans le modèle. Ainsi, quand on vérifiera à quel groupe un point appartient, il sera cohérent avec le groupe dans `y`. Sinon, le modèle trouvera les pics dans un ordre aléatoire et rien ne l'empêchera de changer leur ordre.\n",
    "\n",
    "**Exercice: Utilisez `GaussianMixture` pour modéliser les données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86610159",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09965661",
   "metadata": {},
   "source": [
    "Une fois le modèle ajusté, on peut explorer ses attributs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fd1728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Poids de chaque distribution:\", gm.weights_)\n",
    "print(\"Moyennes:\", gm.means_)\n",
    "print(\"Covariances:\", gm.covariances_)\n",
    "print(\"Covariances shape:\", gm.covariances_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54c9135",
   "metadata": {},
   "source": [
    "On a donc:\n",
    "\n",
    "- Le poids de chaque distribution (pic) dans le modèle\n",
    "- La moyenne de chaque distribution\n",
    "- Les covariances, dont le format dépend de `covariance_type`. Selon la documentation, on aura:\n",
    "    - (n_components,)                        if 'spherical',\n",
    "    - (n_features, n_features)               if 'tied',\n",
    "    - (n_components, n_features)             if 'diag',\n",
    "    - (n_components, n_features, n_features) if 'full'\n",
    "\n",
    "Comme notre modèle est une distribution normale avec plusieurs mode, on peut calculer la log-vraisemblance des données directement avec la méthode `.score_samples()` du modèle:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea2cce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def get_mesh(X):\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_range = X_max - X_min\n",
    "    X_start = X_min - X_range * 0.1\n",
    "    X_end = X_max + X_range * 0.1\n",
    "    x1, x2 = np.linspace(X_start, X_end, num=1000).T\n",
    "    x1_grid, x2_grid = np.meshgrid(x1, x2)\n",
    "    X_grid = np.array([x1_grid.ravel(), x2_grid.ravel()]).T\n",
    "    return X_grid, x1_grid, x2_grid\n",
    "\n",
    "X_grid, x1_grid, x2_grid = get_mesh(X)\n",
    "log_like = -gm.score_samples(X_grid)\n",
    "log_like = log_like.reshape(x1_grid.shape)\n",
    "\n",
    "CS = plt.contourf(\n",
    "    x1_grid, x2_grid, log_like, norm=colors.LogNorm(vmin=1.0, vmax=1000.0), levels=np.logspace(0, 3, 10)\n",
    ")\n",
    "CB = plt.colorbar(CS, label=\"Log L\")\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], color=\"k\", alpha=0.3, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Données et contours de log-vraisemblance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb21c33d",
   "metadata": {},
   "source": [
    "On peut également vérifier la qualité du modèle avec le BIC, que nous avons vu plus tôt dans le cours. Ceci pourrait être utile afin de comparer rapidement deux modèles avec des nombres de pics différents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35248380",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BIC\", gm.bic(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131fbb06",
   "metadata": {},
   "source": [
    "En plus d'une méthode `.fit()`, les modèles `sklearn` ont généralement une méthode `.predict()` qui permet d'utiliser le modèle pour l'inférence.\n",
    "Ainsi, on peut prédire le groupe (la distribution) pour les points d'entraînement `X` ou pour de nouveaux points.\n",
    "\n",
    "**Remarque: Si nous ne connaissions pas la moyenne à l'avance, nous n'aurions pas pu spécifier `means_init` ci-dessus. Ainsi, le pic 1 dans les données aurait pu avoir un autre indice dans le résultat du modèle. La précision (en %) ci-dessous serait alors invalide.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57907e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gm.predict(X)\n",
    "good_mask = y_pred == y\n",
    "accuracy = np.sum(good_mask) / len(y) * 100\n",
    "print(\"Accuracy\", accuracy, \"%\")\n",
    "\n",
    "plt.scatter(X[good_mask, 0], X[good_mask, 1], c=y_pred[good_mask], cmap=cmap, alpha=1.0, edgecolor=\"k\", label=\"Good\")\n",
    "plt.scatter(X[~good_mask, 0], X[~good_mask, 1], c=y_pred[~good_mask], marker=\"x\", cmap=cmap, alpha=1.0, label=\"Bad\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Classe prédite pour chaque point\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d51cf04",
   "metadata": {},
   "source": [
    "On peut également afficher la covariance des distributions avec une ellipse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96125416",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "def make_ellipses(gmm: GaussianMixture, ax=None, colors=None):\n",
    "    \"\"\"\n",
    "    Affichage d'une ellipse à partir d'un GMM\n",
    "    Ref: https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html\n",
    "    \"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    if colors is None:\n",
    "        colors = [f\"C{i}\" for i in range(gmm.n_components)]\n",
    "    for n, color in enumerate(colors):\n",
    "        if gmm.covariance_type == \"full\":\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == \"tied\":\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == \"diag\":\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == \"spherical\":\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2.0 * np.sqrt(2.0) * np.sqrt(v)\n",
    "        ell = Ellipse(\n",
    "            gmm.means_[n, :2], v[0], v[1], angle=180 + angle, color=color\n",
    "        )\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.8)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect(\"equal\", \"datalim\")\n",
    "\n",
    "plt.scatter(X[good_mask, 0], X[good_mask, 1], c=y_pred[good_mask], cmap=cmap, alpha=1.0, edgecolor=\"k\", label=\"Good\")\n",
    "make_ellipses(gm, colors=cmap.colors)\n",
    "plt.scatter(X[~good_mask, 0], X[~good_mask, 1], c=y_pred[~good_mask], marker=\"x\", cmap=cmap, alpha=1.0, label=\"Bad\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Classe prédite pour chaque point\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ab5dd",
   "metadata": {},
   "source": [
    "On pourrait finalement simuler de nouvelles données à partir de notre modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc83b9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "plt.scatter(X[good_mask, 0], X[good_mask, 1], c=y_pred[good_mask], cmap=cmap, alpha=1.0, edgecolor=\"k\", label=\"Good\")\n",
    "make_ellipses(gm, colors=cmap.colors)\n",
    "plt.scatter(X[~good_mask, 0], X[~good_mask, 1], c=y_pred[~good_mask], marker=\"x\", cmap=cmap, alpha=1.0, label=\"Bad\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Classe prédite pour chaque point\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(122)\n",
    "X_samples, y_samples = gm.sample(n_samples=1000)\n",
    "plt.scatter(X_samples[:, 0], X_samples[:, 1], c=y_samples, cmap=cmap, alpha=1.0, edgecolor=\"k\", label=\"Good\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Échantillons générés par le modèle\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf34c67-0acb-4aaa-b23b-25392f8cd432",
   "metadata": {},
   "source": [
    "- **Exercice: Testez l'exemple ci-dessus avec une autre distribution, par exemple `5-random`. N'hésitez pas à modifier l'écart type ou la position des pics pour voir l'impact sur le modèle.** \n",
    "- **Exercice: Utilisez un nombre de distributions plus petit ou plus grand que le nombre réel utilisé pour générer les données. L'arguemnt `mean_init` fonctionne-t-il encore? Comment la prédiction est-elle affectée**\n",
    "- **Exercice: Testez les différentes méthodes pour la covariance sur l'esemble de données `1-diag-1-circle`, puis sur l'ensemble `2-diag`. Dans chaque cas, lequel semble plus approprié?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966b410f",
   "metadata": {},
   "source": [
    "## Méthode des $k$-moyennes\n",
    "\n",
    "La mixture gaussienne peut être vue comme une forme de partitionnement (_clustering_).\n",
    "scikit-learn [implémente](https://scikit-learn.org/stable/modules/clustering.html) plusieurs méthodes de partitionnement.\n",
    "Ici, nous explorerons uniquement l'une des formes les plus simple (mais tout de même très commune), soit la méthode des $k$-moyennes (_k-means clustering_ en anglais).\n",
    "\n",
    "On peut réutiliser la même fonction pour simuler des données 2D et appliquer la méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75856e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_dist = \"5-random\"\n",
    "X, y, centers, N = generate_data(N_samples, kmeans_dist)\n",
    "\n",
    "cmap = discrete_cmap(N)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap, alpha=1.0, edgecolor=\"k\")\n",
    "plt.colorbar(ticks=range(N), label=\"Groupe\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.clim(-0.5, N-0.5)\n",
    "plt.title(\"Données affichées avec leur vrai groupe\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28e57a0",
   "metadata": {},
   "source": [
    "On voit qu'avec cette distribution, les groupes ne sont pas toujours bien séparés.\n",
    "\n",
    "On peut tout de même tester le partitionnement.\n",
    "Encore une fois, `sklearn` implémente le gros du travail pour nous!\n",
    "\n",
    "Ici, `init` nous permet encore une fois d'avoir la bonne correspondance entre les classes (groupes) réelles et prédites.\n",
    "\n",
    "**Exercice: Utilisez KMeans pour modéliser les données**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be72361",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# TODO: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f80414-eacb-4c53-b940-4dc2d379dc6e",
   "metadata": {},
   "source": [
    "Les classes `sklearn` ont des méthodes `fit_predict` et `fit_transform` permettant d'ajuster le modèle et de retourner une prédiction en même temps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552e468",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_mask = y_pred == y\n",
    "accuracy = np.sum(good_mask) / len(y) * 100\n",
    "print(\"Accuracy\", accuracy, \"%\")\n",
    "\n",
    "cmap = discrete_cmap(N_clusters)\n",
    "plt.scatter(X[good_mask, 0], X[good_mask, 1], c=y_pred[good_mask], cmap=cmap, alpha=1.0, edgecolor=\"k\", label=\"Good\")\n",
    "plt.scatter(X[~good_mask, 0], X[~good_mask, 1], c=y_pred[~good_mask], marker=\"x\", cmap=cmap, alpha=1.0, label=\"Bad\")\n",
    "plt.colorbar(ticks=range(N_clusters), label=\"Groupe\")\n",
    "plt.clim(-0.5, N_clusters-0.5)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.title(\"Classe prédite pour chaque point\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87317377",
   "metadata": {},
   "source": [
    "On peut également afficher le centre de chaque groupe et les frontières avec une grille.\n",
    "On peut définir une grille et prédire le groupe de chaque point.\n",
    "Ensuite, on peut obtenir le centre de chaque groupe avec `km.cluster_centers_`.\n",
    "\n",
    "**Exercice: Utilisez `get_mesh`, `km.predict()` et `plt.contourf` pour afficher les groupes. Utilisez ensuite `km.cluster_centers_` pour afficher leur centre**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839dbfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ed1410",
   "metadata": {},
   "source": [
    "## Analyse de composantes principales (PCA)\n",
    "\n",
    "L'analyse de composantes principales (PCA) est elle aussi implémentée dans `sklearn`.\n",
    "Généralement avec une PCA, on ne cherche pas à trouver des groupes dans les données, mais plutôt à réduire les dimensions de nos données pour extraire les directions les plus importantes.\n",
    "\n",
    "On peut faire un exemple simple avec des données gaussiennes corrélées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f5c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "var = 3\n",
    "corr = -0.8\n",
    "\n",
    "cov_mat = [[var, corr * var], [corr * var, var]]\n",
    "X = np.random.multivariate_normal([0, 0], cov_mat, 100)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=1.0, edgecolor=\"k\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1189edd",
   "metadata": {},
   "source": [
    "On doit spécifier le nombre de composantes en entrée à notre objet `PCA`.\n",
    "Ici, on a deux dimensions donc on peut utiliser une ou deux composantes.\n",
    "Deux nous donne une reconstruction parfaite et ne réduit pas la dimension du problème.\n",
    "Elle permet de capturer les deux directions principales dans les données.\n",
    "\n",
    "Par défaut, `PCA()` utilisera le plus de dimensions possible, donc on aurait pu omettre `n_components`.\n",
    "\n",
    "**Exercice: Ajustez une PCA aux données et transformez les dans un tableau `X_proj`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a97eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# TODO: PCA fit and transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb93d01b",
   "metadata": {},
   "source": [
    "Contrairement à `GaussianMixture` et `KMeans`, `PCA` ne définit pas un prédicteur, mais plutôt une transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3babd9-440f-493e-975d-f4a22ee1324a",
   "metadata": {},
   "source": [
    "Maintenant que `fit_transform()` a été utilisé, on peut visualiser les composantes avec `pca.components_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef21de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=1.0, edgecolor=\"k\")\n",
    "for i in range(n_pc):\n",
    "    plt.annotate(\"\", xytext=(0, 0), xy=pca.components_[i], arrowprops={\"arrowstyle\": \"simple\", \"facecolor\": \"w\", \"edgecolor\": \"k\"}, size=15)\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"equal\")\n",
    "plt.title(\"Données et composantes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82bcbb9-4555-41dd-937d-ed3e3d739069",
   "metadata": {},
   "source": [
    "Dans `X_proj`, chaque point a été transformé dans les coordonnées alignées avec les composantes principales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dde33c-fe45-4aa7-9801-4dad6f6d3b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_proj[:, 0], X_proj[:, 1], alpha=1.0, edgecolor=\"k\")\n",
    "plt.title(\"Données projetées par la PCA\")\n",
    "plt.xlabel(\"$x_1'$\")\n",
    "plt.ylabel(\"$x_2'$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ad8d38",
   "metadata": {},
   "source": [
    "Si on souhaite condenser les données sur une seule composante, on peut procéder ainsi:\n",
    "\n",
    "1. PCA avec une seule composante\n",
    "2. On ajuste aux données\n",
    "3. On transforme les données\n",
    "4. On fait la transformée inverse. Comme les données transformées condensent l'information, les données reprojetées seront sur une seule ligne.\n",
    "\n",
    "**Exercice: Effectuez les étapes ci-dessus. Affichez ensuite les données et leur version reprojetée.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c74be03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PCA n=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc694af-6917-40ca-aecc-79c912e39819",
   "metadata": {},
   "source": [
    "## Images\n",
    "\n",
    "Jusqu'ici nous avons travaillé uniquement avec des données 2D.\n",
    "Elles sont faciles à visualiser et donc pratiques pour se familiariser avec les méthodes ci-dessus, mais les données réelles sont souvent plus complexes.\n",
    "Une série temporelle aura souvent plusieurs centaines de points, et une image plusieurs dizaines ou centaines de pixels.\n",
    "\n",
    "Pour le reste du notebook, nous allons travailler avec des images.\n",
    "Commençons avec des images de chiffres écrit à la main: elles sont faciles à interpréter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158fb104",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "X, y = digits.data, digits.target\n",
    "X_full = X.copy()\n",
    "N_digits = len(X)\n",
    "# Décommenter pour utiliser uniquement l'un des chiffres\n",
    "# idx = y == 9\n",
    "# X = X[idx]\n",
    "# y = y[idx]\n",
    "w = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e64e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng()\n",
    "i = rng.integers(N_digits)\n",
    "plt.imshow(X[i].reshape(w, w))\n",
    "plt.title(f\"Exemple de chiffre: {y[i]}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c562ceb-2748-4ae3-b54c-9f0eec1131b8",
   "metadata": {},
   "source": [
    "### PCA\n",
    "\n",
    "On peut appliquer l'analyse de composantes principales à nos images pour voir ce qui en est extrait.\n",
    "C'est généralement une bonne idée de recentrer les données autour de la moyenne pour faciliter l'apprentissage.\n",
    "\n",
    "**Exercice: Extrayez une image moyenne et soustrayez la aux données. Affichez l'image moyenne et un exemple recentré**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f77cb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51723f24-e8e7-4166-8199-cf076171e883",
   "metadata": {},
   "source": [
    "On peut maintenant appliquer l'analyse de composantes principales.\n",
    "\n",
    "Commençons avec 10 composantes pour voir les principales informations apprises.\n",
    "\n",
    "**Exercice: Appliquez une PCA avec 10 composantes. Affichez les 10 \"eigen-images\" et un exemple d'image reconstruite**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045b786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exercice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3c22f5",
   "metadata": {},
   "source": [
    "### PCA avec autant de composantes que de pixels\n",
    "\n",
    "Il n'est pas toujours clair quel est le nombre de composantes optimal.\n",
    "Il existe différente métrique pour évaluer la performance sur des données test, mais on peut d'abord visualiser le nombre maximal de composantes pour avoir un peu d'intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba96a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "X_proj = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480acbb9-9f1f-444d-93d1-02c1a4bfe93c",
   "metadata": {},
   "source": [
    "On peut d'abord visualiser la variance expliquée en fonction du nombre de composantes.\n",
    "On voit que les quelques premières composantes (10-20) encapsulent une grande partie des variations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b05b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n",
    "axs[0].plot(pca.explained_variance_ratio_)\n",
    "axs[0].set_xlabel(\"Nombre de composantes\")\n",
    "axs[0].set_ylabel(\"Ratio de la variance expliquée\")\n",
    "axs[1].plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "axs[1].set_xlabel(\"Nombre de composantes\")\n",
    "axs[1].set_ylabel(\"Ratio de la variance expliquée (cumulatif)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a8cb10f-5d42-4282-87b6-68cfe01d0a42",
   "metadata": {},
   "source": [
    "On peut également afficher les _eigen-images_.\n",
    "On voit que les dernières images ne concernent que quelques pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04da441-15ac-41cb-8671-3e8a5eee29e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(8, 8, figsize=(20,20))\n",
    "axs = axs.ravel()\n",
    "for i in range(pca.n_components_):\n",
    "    axs[i].imshow(pca.components_[i].reshape(w, w))\n",
    "    axs[i].set_title(f\"Composante {i}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7882f99",
   "metadata": {},
   "source": [
    "### PCA 2D et K-moyennes\n",
    "\n",
    "On peut appliquer une PCA et ensuite la méthode des k-moyennes. Ceci permet de d'abord compresser les données puis les grouper.\n",
    "\n",
    "Comme on a accès aux catégories dans ce cas-ci, on peut choisir un nombre approprié de groupes (10) et donner l'emplacement moyen de chaque groupe comme position initiale.\n",
    "\n",
    "**Exercice: Appliquez une PCA aux images avec 2 composantes. Affichez les données projetées dans cet espace et utilisez une couleur différente pour chaque chiffre (similaire aux graphiques 2D des premières cellules**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90d0e68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: PCA et graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69806e9a-11aa-4a65-bfd0-e17710784894",
   "metadata": {},
   "source": [
    "**Exercice: Appliquez les K-moyennes avec 10 groupes aux images projetées en 2D. Vous pouvez utiliser la moyenne de chaque groupe comme point de départ (init). Affichez la prédiction et la précision comme nous avons fait plus haut. Vous pouvez également afficher la frontière entre les groupes dans un autre graphique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224ccb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Exemple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa2b186-ef98-439b-8ae6-4e7020d42c29",
   "metadata": {},
   "source": [
    "Pour vérifier la qualité des prédictions, en plus du % de précision, on peut utiliser une matrice de confusion.\n",
    "Celle-ci nous donne le nombre de points prédits dans la classe $i$ mais appartenant à la classe $j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e72fa9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_pred, y)\n",
    "plt.imshow(cmat)\n",
    "plt.title(\"Matrice de confusion\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d49b0a",
   "metadata": {},
   "source": [
    "### K-moyennes directement sur les images\n",
    "\n",
    "On peut tester la méthodes de K-moyennes sans PCA pour voir comment l'apprentissage est affecté.\n",
    "On pourrait aussi tester des scénarios intermédiaires avec `n_pca` entre 2 et 63.\n",
    "\n",
    "**Exercice: Appliquez la méthode des K-moyennes directement sur les images. Pas besoin d'afficher un nuage de points comme plus haut, comme les points auront 64 dimensions. Calculez la précision et la matrice de confusion.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d718ed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: K-moyenne"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50a5bc3",
   "metadata": {},
   "source": [
    "## Exercices additionnels\n",
    "\n",
    "- Explorer l'utilisation d'un GMM sur les données d'images ci-dessus\n",
    "- Explorez les méthodes ci-dessus avec l'ensemble de données `fetch_olivetti_faces` de `sklearn.datasets`\n",
    "- Explorez les méthodes ci-dessus avec des images de galaxies utilisées dans le devoir 3"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
