{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1320595",
   "metadata": {},
   "source": [
    "# Processus gaussiens\n",
    "\n",
    "## JAX\n",
    "\n",
    "La librairie que nous allons utiliser pour les GPs, [tinygp](https://tinygp.readthedocs.io/en/stable/index.html), utilise [JAX](https://docs.jax.dev/en/latest/index.html).\n",
    "JAX est une librairie pour l'apprentissage machine, un peu comme PyTorch et Tensorflow, qui permet de tirer avantage des GPUs et TPUs.\n",
    "Nous couvriront ce type de librairies et leurs avantages lors des cours sur les r√©seaux neuronaux.\n",
    "En plus du calcul sur GPU, voici quelques autres avantages de JAX:\n",
    "\n",
    "- Interface similaire √† Numpy (`jax.numpy`)\n",
    "- Calcul de gradients (`jax.grad`)\n",
    "- Compilation _just in time_ (JIT) (`jax.jit`)\n",
    "- Vectorisation automatique (`jax.vmap`)\n",
    "\n",
    "Bien que JAX permette d'utiliser les GPUs, elle fonctionne aussi sur les CPUs.\n",
    "Vous pouvez donc installer JAX sur votre ordinateur en suivant les [instructions d'installation](https://docs.jax.dev/en/latest/installation.html#).\n",
    "\n",
    "Pour offrir les fonctionnalit√©s d√©crites ci-dessus, JAX fonctionne diff√©rement de Numpy √† quelques √©gards.\n",
    "Ces diff√©rences sont pr√©sent√©es dans le tutoriel [üî™ JAX - The Sharp Bits üî™](https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html), mais voici les grandes lignes:\n",
    "\n",
    "- JAX s'attend √† des fonctions ¬´ pures ¬ª, c'est √† dire qui retournent toujours la m√™me valeur pour les m√™mes entr√©es et n'utilisent pas le contexte \"global\"\n",
    "- Les tableaux ne peuvent √™tre modifi√©s directement. `a[0] = 1.0` devient `a = a.at[0].set(1.0)` ([documentation pour la propri√©t√© `at`](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.ndarray.at.html#jax.numpy.ndarray.at))\n",
    "- Les nombre al√©atoires sont g√©n√©r√©s diff√©remment\n",
    "- La pr√©cision par d√©faut de JAX pour les nombres r√©els est 32bit (_single_). Pour utiliser la pr√©cision 64bit (_double_), il faut ajouter `jax.config.update(\"jax_enable_x64\", True)` au d√©but du code.\n",
    "\n",
    "### D√©finition d'un tableau et modification\n",
    "\n",
    "Commen√ßons par d√©finir un tableau avec JAX et v√©rifier que la pr√©cision _double_ est bien activ√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "a = jnp.arange(5.0)\n",
    "print(\"Test\")\n",
    "print(a)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142da2b5",
   "metadata": {},
   "source": [
    "On peut voir que modifier le tableau en place donne une erreur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a689a791",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    a[0] = 1\n",
    "except TypeError as e:\n",
    "    print(\"L'erreur suivante s'est produite!\\n\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654fa12f",
   "metadata": {},
   "source": [
    "Mais que la propri√©t√© `at` fonctionne tel qu'attendu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3017a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.at[0].set(100.0)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792de6eb",
   "metadata": {},
   "source": [
    "### Fonction et diff√©rentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b971ee40",
   "metadata": {},
   "source": [
    "Comme avec Numpy, on peut d√©finir une fonction et l'afficher dans un graphique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61c833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sinus(x, p):\n",
    "    A, P, phi, off = p\n",
    "    return A * jnp.sin(2 * jnp.pi * x / P - phi) + off\n",
    "\n",
    "\n",
    "x = jnp.linspace(0, 5, num=1000)\n",
    "params = [1.0, 1.0, 0.0, 0.0]\n",
    "y = sinus(x, params)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8b33c9",
   "metadata": {},
   "source": [
    "Mais comme la fonction utilise `jax.numpy`, on peut obtenir un gradient avec `jax.grad`.\n",
    "La fonction `jax.grad` accepte seulement les fonctions qui retournent un scalaire.\n",
    "C'est ici que la fonction `jax.vmap` devient utile: elle permet d'appliquer notre fonction de gradient sur plusieurs `x`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49e2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinus_grad = jax.grad(sinus)  # grad se fait sur le 1er argument par d√©faut\n",
    "# on doit dire √† vmap sur quels arguments op√©rer:\n",
    "# - axe 0 du premier argument\n",
    "# - aucun axe du 2e argument\n",
    "yp = jax.vmap(sinus_grad, in_axes=(0, None))(x, params)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label=\"f(x)\")\n",
    "plt.plot(x, yp,label=\"f'(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c2b136",
   "metadata": {},
   "source": [
    "### Compilation _just in time_ (JIT)\n",
    "\n",
    "On peut √©galement cr√©er une version compil√© de notre fonction avec `jax.jit`.\n",
    "Lors de la premi√®re √©valuation, la fonction sera compil√©e et sera ainsi plus rapide les fois suivantes.\n",
    "On peut utiliser `%timeit` pour tester l'acc√©l√©ration.\n",
    "(Ici, `block_until_read()` s'assure seulement que JAX n'acc√©l√®re pas l'√©valuation avec le _dispatch_ asynchrone)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bb4813",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = jnp.linspace(0.0, 10.0, num=1_000_00)\n",
    "%timeit sinus(xtest, params).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c381e129",
   "metadata": {},
   "source": [
    "Si on utilise `jit`, on peut acc√©l√©rer la fonction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "sinus_jit = jax.jit(sinus)\n",
    "_ = sinus_jit(x, params)  # On compile en ex√©cutant une premi√®re fois\n",
    "%timeit sinus_jit(xtest, params).block_until_ready()  # On teste la version compil√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdccca7",
   "metadata": {},
   "source": [
    "Dans ce cas-ci, la fonction est plut√¥t simple et le temps gagn√© n'est pas √©norme.\n",
    "Par contre, plus nos mod√®les sont complexes, plus la compilation JIT sera utile.\n",
    "\n",
    "`jax.jit` peut aussi √™tre utilis√©e comme d√©corateur sur notre fonction, par exemple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c59634a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def sinus_jit_deco(x, p):\n",
    "    A, P, phi, off = p\n",
    "    return A * jnp.sin(2 * jnp.pi * x / P - phi) + off\n",
    "\n",
    "_ = sinus_jit_deco(x, params)  # On compile\n",
    "%timeit sinus_jit_deco(xtest, params).block_until_ready()  # On teste la version compil√©e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdc122c",
   "metadata": {},
   "source": [
    "Ces deux m√©thodes sont √©quivalentes.\n",
    "\n",
    "### Nombres al√©atoires\n",
    "\n",
    "Avec JAX, les nombres al√©atoires sont g√©n√©r√©s un peu diff√©remment d'avec Numpy.\n",
    "Pour op√©rer sur des fonctions ¬´ pures ¬ª, il faut que l'√©tat du g√©n√©rateur al√©atoire soit pass√© en argument aux diff√©rentes fonctions.\n",
    "On fait ceci √† l'aide d'une ¬´ cl√© ¬ª, qui peut √™tre divis√©e autant de fois que n√©cessaire pour g√©n√©rer de nouveaux nombres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a1acb5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "key = jax.random.key(3051)\n",
    "print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ed8e5",
   "metadata": {},
   "source": [
    "Une cl√© donn√©e g√©n√®re toujours la m√™me valeur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951be09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(jax.random.normal(key))\n",
    "print(jax.random.normal(key))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20911f57",
   "metadata": {},
   "source": [
    "Pour obtenir de nouveaux nombres, il faut diviser la cl√© en une nouvelle `key` et une `subkey`.\n",
    "On utilise la `subkey` pour g√©n√©rer des nombres al√©atoires.\n",
    "La nouvelle `key` sera re-divis√©e plus loin dans notre code au besoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03c091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(4):\n",
    "    key, subkey = jax.random.split(key)\n",
    "    print(jax.random.normal(subkey))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b092937-a7fe-4103-a8c4-42fa5ae896de",
   "metadata": {},
   "source": [
    "**Exercice: G√©n√©rez des nombres tir√©es d'une distribution normale 2D centr√©e √† [0, 5] avec une une matrice la matrice de covariance diagonale ci-dessous. Affichez les √©chantillons sur un histogramme 2D (`plt.hist2d` ou `corner.corner`).**\n",
    "\n",
    "$$\n",
    "C = \\begin{bmatrix}1 & 0\\\\ 0 & 2\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "(La distribution normale multidimensionnelle est `multivariate_normal` dans NumPy et JAX)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8e1dde-9f70-4cd3-a503-bd8adb70f924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1712bebb",
   "metadata": {},
   "source": [
    "Voil√†! La section ci-dessus couvrait les principales choses √† savoir avant d'utiliser JAX.\n",
    "\n",
    "## Processus gaussiens (impl√©mentation)\n",
    "\n",
    "Tel que vu en classe, les processus gaussiens (GPs) sont math√©matiquement assez simple.\n",
    "Il est donc possible de les impl√©menter avec `numpy` ou `jax.numpy`.\n",
    "\n",
    "Les √©quations dont nous avons besoin sont donn√©es aux diapositives 12, 15 et 23.\n",
    "\n",
    "### Fonction de covariance\n",
    "\n",
    "Commen√ßons par d√©finir une fonction de covariance (_kernel_) exponentielle carr√©e.\n",
    "\n",
    "$$\n",
    "k_{\\mathrm{SE}}(x_i, x_j, \\{\\lambda\\}) = \\exp{\\left(-\\frac{1}{2\\lambda^2}\\left|x_i - x_j\\right|^2\\right)}\n",
    "$$\n",
    "\n",
    "**Impl√©mentez l'√©quation ci-dessus et affichez sont r√©sultat pour $x_i$ entre -5 et 5. Gardez $x_j$ fixe √† 0.**\n",
    "**Utilisez une √©chelle $\\lambda = 1$.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c137c389",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4083c065",
   "metadata": {},
   "source": [
    "La fonction nous indique que les points √©tant √† pr√®s les uns des autres sont tr√®s corr√©l√©s, mais qu'au del√† de $|x_i - x_j| > 4$, la corr√©lation est presque 0.\n",
    "\n",
    "On peut visualiser cette corr√©lation d'une autre mani√®re, en utilisant deux vecteurs et en regardant la distance point-par-point.\n",
    "\n",
    "**Utilisez 100 valeurs de $x$ entre 0 et 10 dans un tableau `xi` et calculez la matrice de covariance. Vous pouvez utiliser `xi[:, None]` et `xi[None, :]` pour automatiquement ajouter les axes dans le calcul du kernel ou bien utiliser `meshgrid` pour couvrir toutes les combinaisons de points dans `xi`. Enregistrez votre matrice de covariance dans une variable `kmat`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e504d9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: Afficher la matrice"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb81645",
   "metadata": {},
   "source": [
    "### Distribution a priori\n",
    "\n",
    "Comme un GP est une distribution gaussienne sur les fonctions, on peut utiliser la matrice de covariance `kmat` pour g√©n√©rer des √©valuations $\\mathbf{y} = f(\\mathbf{x})$ tir√©es de cette distribution.\n",
    "Notre fonction moyenne dans ce cas-ci sera simplement une constante autour de 0.\n",
    "\n",
    "**Utilisez `jax.random.multivariate_normal()` pour g√©n√©rer 5 √©chantillons d'une distribution avec une moyenne de 0 et une covariance `kmat`.**\n",
    "**Utilisez l'argument `method=\"svd\"` pour √©viter les erreurs num√©riques.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda06fc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "570467a7",
   "metadata": {},
   "source": [
    "La distribution normale √† 100 dimension nous retourne donc 5 vecteurs $y$ √©valu√©s aux valeurs de notre fonction.\n",
    "On peut les afficher sur un m√™me graphique pour voir quel type de fonctions le GP retourne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c053e955",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xi, gp_prior_samples.T)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"√âchantillons a priori du GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8a9e89",
   "metadata": {},
   "source": [
    "Ces √©chantillons sont tir√©es de la distribution **a priori** du GP.\n",
    "Ils ne sont donc conditionn√©s sur aucunes donn√©es, ce qui les rend tr√®s peu utiles en pratique.\n",
    "\n",
    "### Distribution a posteriori (conditionnelle)\n",
    "\n",
    "On peut simuler quelques points de donn√©es et utiliser les √©quations de la page 23 pour obtenir la distribution **a posterori** du GP conditionn√©e sur ces observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8959770",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey1, subkey2 = jax.random.split(key, num=3)\n",
    "x = jax.random.uniform(subkey1, shape=5, minval=0.5, maxval=9.0)\n",
    "y = jax.random.normal(subkey2, shape=x.shape)\n",
    "xt = jnp.linspace(0, 10, num=100)\n",
    "\n",
    "plt.plot(x, y, \"ko\", label=\"Observations\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Observations simul√©es\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69998a4d",
   "metadata": {},
   "source": [
    "Maintenant que nous avons des observations, nous pouvons g√©n√©rer des √©chantillons de la distribution a posteriori (conditionnelle) du GP.\n",
    "Dans ces √©quations $x$ et $y$ d√©notent les vecteurs d'observations.\n",
    "$x_\\star$ est le vecteur points o√π l'on veut pr√©dire $y_\\star$ (`xt` dans le code).\n",
    "La distribution conditionnelle est donn√©e √† la page 23 de diapositives. Il faut d'abord √©valuer trois matrices de covariance\n",
    "\n",
    "$$\n",
    "K = k(x, x), K_\\star = k(x,x_\\star), K_{\\star\\star} = k(x_\\star, x_\\star).\n",
    "$$\n",
    "\n",
    "La distribution conditionnelle d'un GP est aussi une distribution gaussienne multidimensionnelle, mais sa moyenne et sa covariance sont diff√©rentes de la distribution a priori:\n",
    "\n",
    "$$\n",
    "p(y_{\\star}|y) = \\mathcal{N}(f_\\star, C_\\star)\n",
    "$$\n",
    "\n",
    "o√π la moyenne est\n",
    "\n",
    "$$\n",
    "f_\\star = m_\\star + K_{\\star}^T K^{-1} (y - m)\n",
    "$$\n",
    "\n",
    "et la covariance\n",
    "\n",
    "$$\n",
    "C_\\star = K_{\\star\\star} + K_{\\star}^T K^{-1} K_{\\star}.\n",
    "$$\n",
    "\n",
    "- **Utilisez ces √©quations pour obtenir la distribution conditionnelle du GP**. Il s'agit d'une distribution normale √† plusieurs dimensions. Le nombre de dimension est le nombre de points dans ce cas-ci. Les dimensions seront inf√©r√©es automatiquement par Numpy ou JAX lorsque vous passez le vecteur moyenne $f_{\\star}$ et la matrice de covariance $C_{\\star}$ en argument.\n",
    "- **Une fois cette distribution obtenue, affichez la moyenne $f_{\\star}$ superpos√©e aux donn√©es.**\n",
    "- **Affichez √©galement un intervalle `fill_between` autour de la moyenne en utilisant l'√©cart type d√©riv√© de $C_{\\star}$** (indice: la variance se trouve sur la diagonale de $C_{\\star}$).\n",
    "- **Affichez aussi 5 √©chantillons tir√©s de la distribution a posteriori du GP**\n",
    "\n",
    "<details>\n",
    "    \n",
    "<summary>Quelques indices (cliquer pour afficher)</summary>\n",
    "\n",
    "1. Utilisez `kernel(x[:, None], x[None, :])` pour facilement vectoriser le calcul de la matrice de covariance\n",
    "2. Les fonctions `jnp.linalg`, `jnp.diag` et l'op√©ration de multiplication matricielle `@` seront utiles.\n",
    "3. Pour g√©n√©rer les √©chantillons, le code du prior peut √™tre r√©utilis√©, mais il faut changer la moyenne et la covariance. N'oubliez pas de re-diviser la cl√©!\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e6015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pr√©diction GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b567d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphique GP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc68e5f3",
   "metadata": {},
   "source": [
    "## Exploration\n",
    "\n",
    "Si vous terminez d'avance, voici quelques suggestions pour explorer les GPs un peu plus en d√©tail.\n",
    "Rien de ceci n'est obligatoire.\n",
    "\n",
    "Quelques suggestions:\n",
    "\n",
    "- Tentez de modifier l'hyperparam√®tre $\\lambda$ comment cela affecte-t-il vos r√©sultats?\n",
    "- Testez d'autres fonction de covariances\n",
    "- Ajoutez des barres d'erreur aux observations. Ajoutez la variance correspondante √† la diagonale de votre matrice $K = k(x,x)$.\n",
    "- Impl√©mentez l'√©quation du _likelihood_ (diapositive 28) pour le GP et tentez d'optimiser l'hyperparam√®tre pour ce probl√®me simple.\n",
    "- Comment se comporte le temps de calcul du GP en ajoutant des observations?\n",
    "- Tentez d'impl√©menter l'algorithme donn√© √† la diapositive 33. La factorisation de Cholesky est impl√©ment√©e dans [NumPy](https://numpy.org/doc/2.2/reference/generated/numpy.linalg.cholesky.html) et [JAX](https://docs.jax.dev/en/latest/_autosummary/jax.numpy.linalg.cholesky.html).\n",
    "- Commencez √† explorer la documentation de [tinygp](https://tinygp.readthedocs.io/en/stable/index.html).\n",
    "\n",
    "**C'est possible que vous obteniez des instabilit√©s num√©riques aux 4 premi√®res questions. Ne perdez pas trop de temps l√† dessus si c'est le cas. C'est entre autre pourquoi nous utiliserons tinygp lors du prochain cours**."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
