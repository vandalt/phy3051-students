{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acedc8a5",
   "metadata": {},
   "source": [
    "# Processus gaussiens avec tinygp\n",
    "\n",
    "Maintenant que nous avons exploré les GPs avec une implémentation à la main, il est temps d'utiliser une librairie plus robuste.\n",
    "Nous utiliserons `tinygp`, qui est implémentée avec JAX.\n",
    "\n",
    "Nous allons simplement répliquer un modèle simple comme celui du notebook précédent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66df3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3bfea",
   "metadata": {},
   "source": [
    "## Fonction de covariance\n",
    "\n",
    "Commençons par définir une fonction de covariance (_kernel_) exponentielle carrée.\n",
    "\n",
    "$$\n",
    "k_{\\mathrm{SE}}(x_i, x_j, \\{\\lambda\\}) = \\exp{\\left(-\\frac{1}{2\\lambda^2}\\left|x_i - x_j\\right|^2\\right)}\n",
    "$$\n",
    "\n",
    "Le module `tinygp.kernels` contient les fonctions de covariance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8a9006",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "from tinygp import kernels\n",
    "\n",
    "kernel = kernels.ExpSquared(1.0)\n",
    "\n",
    "xi = jnp.linspace(-5, 5, num=100)\n",
    "xj = jnp.array([0.0])  # les kernels attendent un tableau, pas un scalaire\n",
    "k = kernel(xi, xj)\n",
    "\n",
    "plt.plot(xi - xj, k)\n",
    "plt.xlabel(\"$x_i - x_j$\")\n",
    "plt.ylabel(\"$k((x_i,x_j)$\")\n",
    "plt.title(r\"Fonction de covariance exponentielle carrée avec $\\lambda=1.0$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6aa4c0",
   "metadata": {},
   "source": [
    "On peut ensuite afficher la matrice de covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d686c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = jnp.linspace(0, 10, num=100)\n",
    "kmat = kernel(xi, xi)  # kernel vectorise automatiquement sur les deux axes\n",
    "plt.pcolormesh(xi, xi, kmat)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.title(r\"Matrice de covariance $k(\\mathbf{x},\\mathbf{x})$\")\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b086408",
   "metadata": {},
   "source": [
    "## Distribution a priori\n",
    "\n",
    "Dans `tinygp`, les GPs sont définis par un objet `GaussianProcess`.\n",
    "On spécifie une petite variance sur la diagonale pour éviter les erreurs numérique.\n",
    "Par défaut, la moyenne du GP est de 0.\n",
    "\n",
    "On peut ensuite utiliser la méthode `GaussianProcess.sample` pour obtenir des échantillons du prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d98de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygp import GaussianProcess\n",
    "\n",
    "gp = GaussianProcess(kernel, xi, diag=1e-5)\n",
    "gp_prior_samples = gp.sample(jax.random.key(8), shape=(5,))\n",
    "\n",
    "plt.plot(xi, gp_prior_samples.T)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Échantillons a priori du GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e335d1d",
   "metadata": {},
   "source": [
    "### Distribution a posteriori (conditionnelle)\n",
    "\n",
    "Comme avec notre exemple précédent, on peut simuler des observations et conditionner notre GP sur ces dernières.\n",
    "\n",
    "**Testez différentes valeur pour l'hyperparamètre du kernel pour tester son impact sur les échantillons**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e6bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "key, subkey1, subkey2 = jax.random.split(jax.random.key(514), num=3)\n",
    "x = jax.random.uniform(subkey1, shape=5, minval=0.5, maxval=9.0)\n",
    "y = jax.random.normal(subkey2, shape=x.shape)\n",
    "xt = jnp.linspace(0, 10, num=100)\n",
    "\n",
    "plt.plot(x, y, \"ko\", label=\"Observations\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Observations simulées\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d88a7",
   "metadata": {},
   "source": [
    "Pour conditionner le GP, on peut utiliser la méthode `GaussianProcess.condition`.\n",
    "Cette dernière retourne un objet `ConditionResult` avec un argument `log_probability` (la vraisemblance du GP conditionné) et le GP conditionné.\n",
    "On peut utiliser ce dernier pour générer des échantillons de la distribution a posteriori.\n",
    "\n",
    "**Testez différentes valeurs pour `diag` afin de voir son impact sur la prédiction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b499ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = GaussianProcess(kernel, x, diag=1e-4)\n",
    "cond_logprob, cond_gp = gp.condition(y, xt)\n",
    "fs = cond_gp.mean\n",
    "f_std = jnp.sqrt(cond_gp.variance)\n",
    "gp_post_samples = cond_gp.sample(jax.random.key(3), shape=(5,))\n",
    "\n",
    "plt.plot(x, y, \"ko\", label=\"Observations\")\n",
    "plt.plot(xt, fs, label=\"GP mean\", zorder=10)\n",
    "plt.fill_between(xt, fs - f_std, fs + f_std, alpha=0.3, label=\"Écart-type du GP\")\n",
    "plt.plot(xt, gp_post_samples.T, \"--\", label=[\"Échantillons du GP\"] + [None] * gp_post_samples[:-1].shape[0], alpha=0.8)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution a posteriori (conditionnelle) du GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f01419",
   "metadata": {},
   "source": [
    "## Fonction moyenne\n",
    "\n",
    "Les GPs définis avec tinygp acceptent une fonction moyenne.\n",
    "C'est utile pour définir un modèle physique autour duquel le GP génèrera du bruit.\n",
    "Voici un exemple simple avec une droite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11856d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tinygp import GaussianProcess\n",
    "\n",
    "def mean_function(x):\n",
    "    return 2.0 * x + 1.0\n",
    "\n",
    "gp = GaussianProcess(kernel, xi, diag=1e-5, mean=mean_function)\n",
    "gp_prior_samples = gp.sample(jax.random.key(246), shape=(5,))\n",
    "\n",
    "plt.plot(xi, gp_prior_samples.T)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Échantillons a priori du GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a37307",
   "metadata": {},
   "source": [
    "## Vraisemblance et optimisation des hyperparamètres\n",
    "\n",
    "Pour optimiser les hyperparamètres d'un GP, on utilisera typiquement l'attribut `log_probability`.\n",
    "Pour tirer avantage de la compilation just-in-time avec JAX, il est commun de construire le GP dans une fonction et d'utiliser cette fonction dans une autre fonction `loss` qui définit la fonction \"objectif\" à minimiser (log-likelihood négatif, typiquement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9c1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gp(params, x):\n",
    "    kernel = kernels.ExpSquared(\n",
    "        jnp.exp(params[\"log_scale\"])\n",
    "    )\n",
    "    return GaussianProcess(kernel, x, diag=jnp.exp(params[\"log_diag\"]))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def neg_log_likelihood(params, x, y):\n",
    "    gp = build_gp(params, x)\n",
    "    return -gp.log_probability(y)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"log_scale\": 0.0,\n",
    "    \"log_diag\": -1.0,\n",
    "}\n",
    "neg_log_likelihood(params, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4772c3f",
   "metadata": {},
   "source": [
    "La librairie `jaxopt` a une interface similaire à scipy pour l'optimisation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac136b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaxopt\n",
    "\n",
    "solver = jaxopt.ScipyMinimize(fun=neg_log_likelihood)\n",
    "soln = solver.run(params, x, y)\n",
    "print(f\"Final negative log likelihood: {soln.state.fun_val}\")\n",
    "print(f\"Final parameters {soln.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4cf7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gp = build_gp(soln.params, x)\n",
    "xt = jnp.linspace(0, 10, num=1000)\n",
    "cond_logprob, cond_gp = gp.condition(y, xt)\n",
    "fs = cond_gp.mean\n",
    "f_std = jnp.sqrt(cond_gp.variance)\n",
    "gp_post_samples = cond_gp.sample(jax.random.key(3), shape=(5,))\n",
    "\n",
    "plt.plot(x, y, \"ko\", label=\"Observations\")\n",
    "plt.plot(xt, fs, label=\"GP mean\", zorder=10)\n",
    "plt.fill_between(xt, fs - f_std, fs + f_std, alpha=0.3, label=\"Écart-type du GP\")\n",
    "plt.plot(xt, gp_post_samples.T, \"--\", label=[\"Échantillons du GP\"] + [None] * gp_post_samples[:-1].shape[0], alpha=0.8)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.title(\"Distribution a posteriori (conditionnelle) du GP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e12ee",
   "metadata": {},
   "source": [
    "Ici le fit n'est pas particulièrement bon, entre autres car on a peu de données, mais il montre tout de même la base pour optimiser un modèle GP avec tinygp."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
