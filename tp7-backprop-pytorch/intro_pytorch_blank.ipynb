{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77cc5959-9562-406d-82ef-fcd268b8b9e0",
   "metadata": {},
   "source": [
    "# Introduction à PyTorch\n",
    "\n",
    "Dans ce notebook, nous allons couvrir les bases de [PyTorch](https://pytorch.org/).\n",
    "PyTorch est une librairie de tenseurs, comme JAX, que nous avons utilisé plus tôt dans le cours.\n",
    "Par rapport à JAX, qui a un écosystème très intéressant pour l'inférence Bayésienne, PyTorch a un écosystème de _deep learning_ beaucoup plus mature, stable et complet.\n",
    "Nous utiliserons PyTorch pour l'apprentissage profound et les réseaux neuronaux dans le reste du cours.\n",
    "Il y a plusieurs avantages à utiliser une telle librairie lorsqu'on travaille avec des réseaux neuronaux artificiels, par exemple:\n",
    "\n",
    "- Calcul des dérivées via la rétropropagation\n",
    "- Utilisation de GPUs\n",
    "- Plusieurs blocs de base requis pour implémenter différents modèles\n",
    "- Plusieurs modèles sont déjà implémentés et disponibles en ligne\n",
    "\n",
    "Tout comme avec JAX, l'interface PyTorch n'est pas trop loin de celle de NumPy dans plusieurs cas.\n",
    "Il y a tout de même certaines subtilités que nous allons couvrir plus bas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c535c0-a2ea-46d5-8d51-e2459b786ac3",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "Si ce n'est pas fait, il faut installer PyTorch. Rendez-vous sur [le site web](https://pytorch.org/). Plus bas sur la page il y a une section \"Install PyTorch\".\n",
    "Sélectionnez les options et copiez la commande fournie à côté de \"_Run this command_\".\n",
    "À moins que votre ordinateur n'ait un GPU pour le machine learning, cochez \"CPU\" dans _Compute Platform_.\n",
    "Sur [Google Colab](https://colab.research.google.com/), vous pouvez utiliser CUDA 12.4 (voir [ce tutoriel](https://pytorch.org/tutorials/beginner/colab.html) pour plus de détails)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de84ea35-e507-4b2e-ab2a-890fa28dcaf6",
   "metadata": {},
   "source": [
    "## Tenseurs\n",
    "\n",
    "Habituellement, nous manipulons des ensembles de données via des tableaux NumPy (`np.array`).\n",
    "Avec PyTorch, la structure équivalente est une _tenseur_ (`torch.tensor`)\n",
    "\n",
    "### Création\n",
    "\n",
    "On peut créer les tenseur à partir de listes imbriquées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35211528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "data = [[1, 2], [3, 3]]\n",
    "data_tensor = torch.tensor(data)\n",
    "print(data_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347f77f0",
   "metadata": {},
   "source": [
    "Ici, les données sont copiées vers le tenseur: modifier la liste ne modifiera pas le tenseur et vice-versa.\n",
    "\n",
    "On peut également créer notre tenseur à partir d'un tableau NumPy en utilisant `torch.from_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2481f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_array = np.array(data)\n",
    "data_tensor_np = torch.from_numpy(data_array)\n",
    "print(data_tensor_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8be74e",
   "metadata": {},
   "source": [
    "Quand on crée le tenseur à partir de NumPy, **les objets sont liés en mémoire**.\n",
    "Ceci veut dire qu'une modification au tableau NumPy affectera aussi le tenseur PyTorch.\n",
    "\n",
    "On peut créer une copie pour que les deux objets ne soient plus liés.\n",
    "`torch.Tensor.clone()` est l'équivalent de `np.ndarray.copy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40051e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "data_tensor_cloned = data_tensor_np.clone()\n",
    "data_array[0, 0] = 100\n",
    "print(data_array)  # modifié\n",
    "print(data_tensor_np)  # modifié\n",
    "print(data_tensor_cloned)  # original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56653db0",
   "metadata": {},
   "source": [
    "La majorité des options pour créer un tableau NumPy sont répliquées par PyTorch. Que ce soit avec des constantes ou des nombres aléatoires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733eaad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros(2, 3)\n",
    "ones = torch.ones_like(zeros)\n",
    "rand_vals = torch.rand_like(ones)  # Uniforme [0, 1)\n",
    "rand_norm = torch.randn_like(ones)  # Distribution normale\n",
    "rand_ints = torch.randint_like(ones, 10)  # Entiers [0, 10)\n",
    "\n",
    "print(\"Zeros:\", zeros)\n",
    "print(\"Zeros shape:\", zeros.shape)\n",
    "print(\"Ones:\", ones)\n",
    "print(\"Uniforme:\", rand_vals)\n",
    "print(\"Normal:\", rand_norm)\n",
    "print(\"Entiers:\", rand_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f01b073",
   "metadata": {},
   "source": [
    "### Opérations\n",
    "\n",
    "Les opérations mathématiques sont effectuées de manière similaire à NumPy: élément par élément."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160612d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ones * 5)\n",
    "print(rand_norm * rand_ints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac0f16d",
   "metadata": {},
   "source": [
    "Les opérations logiques fonctionnent également, mais il faut convertir vers un type `bool`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fb0682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erreur:\n",
    "try:\n",
    "    print(ones | zeros)\n",
    "except Exception as e:\n",
    "    print(f\"Erreur détectée: {e}\")\n",
    "\n",
    "# On peut changer le type\n",
    "print(ones.to(torch.bool) | zeros.to(torch.bool))\n",
    "print(ones.to(torch.bool) & zeros.to(torch.bool))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab29c6c",
   "metadata": {},
   "source": [
    "On peut aussi convertir un tensor vers un tableau NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc8ddf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(rand_vals))\n",
    "print(rand_vals.numpy())\n",
    "print(type(rand_vals.numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab6ec8a",
   "metadata": {},
   "source": [
    "### Matplotlib\n",
    "\n",
    "Les tenseurs PyTorch sont directement compatibles avec Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc7cfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "\n",
    "x = torch.arange(10)\n",
    "y = 3 * x + 1\n",
    "plt.plot(x, y)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Graphique avec PyTorch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ab0228",
   "metadata": {},
   "source": [
    "### Utilisation de GPUs\n",
    "\n",
    "Comme mentionné plus haut, un des principaux avantages des tenseurs PyTorch est leur compatibilité avec les GPUs,\n",
    "qui permettent d'accélérer les calculs grâce à la parallélisation.\n",
    "Si comme moi votre ordinateur n'a pas accès à un GPU, la cellule suivante affichera \"`Pas de GPU :(`\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fca0f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si on avait un GPU, on pourrait envoyer le tenseur sur les GPU\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Copié sur CUDA\")\n",
    "    x_data_gpu = data_tensor.to(\"cuda\")\n",
    "else:\n",
    "    print(\"Pas de GPU :(\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb35f4a4",
   "metadata": {},
   "source": [
    "Faire cette petite opération pour chaque tenseur n'est pas très pratique...\n",
    "Une façon simple de ne vérifier qu'une seule fois la disponibilité d'un GPU est de créer une variable `device` qui garde cette information.\n",
    "Ensuite, on peut créer les tenseurs en leur assignant une _device_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fb9394",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut spécifier une seule fois et utiliser ensuite\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# On peut spécifier le GPU à la création au lieu d'utiliser `.to()`\n",
    "x_with_device = torch.rand(2, 2, device=device)\n",
    "# Ou encore envoyer un tenseur existant\n",
    "data_tensor.to(device)\n",
    "\n",
    "print(x_with_device)\n",
    "print(x_with_device.device)\n",
    "print(x_with_device.is_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f240534",
   "metadata": {},
   "source": [
    "## Autodérivation avec `torch.autograd`\n",
    "\n",
    "Comme nous avons avec JAX, un avantage des libraires de tenseur est le calcul automatique des gradients via l'autodérivation.\n",
    "Pour l'apprentissage profond, cette fonctionnalité est essentielle lors de l'entraînement.\n",
    "L'autodérivation et la rétropropagation sont disponibles dans le module `autograd` de PyTorch.\n",
    "\n",
    "### Suivre les gradients avec `requires_grad` et `grad_fn`\n",
    "\n",
    "Par défaut, un tenseur créé avec directement avec `torch` ne gardera pas de trace des gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434ee3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_nograd = torch.linspace(0.0, 2 * torch.pi, steps=25)  # pas de gradient\n",
    "print(\"x_nograd:\", x_nograd)\n",
    "print(\"Requires grad:\", x_nograd.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386d2494",
   "metadata": {},
   "source": [
    "Il faut activer `autograd` explicitement avec `requires_grad=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08955d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(0.0, 1.0, steps=50, requires_grad=True)\n",
    "print(\"x:\", x)\n",
    "print(\"Requires grad:\", x.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a587da9e",
   "metadata": {},
   "source": [
    "Ensuite, toutes les opérations effectuées sur `x` tracerons le gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276fad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "phase = 2 * torch.pi * x\n",
    "print(\"phase:\", phase)\n",
    "print(\"phase.grad_fn:\", phase.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1bfb4c",
   "metadata": {},
   "source": [
    "On voit ici que PyTorch a enregistré que la fonction donnant le gradient pour `y` est une multiplication.\n",
    "La même chose se produit pour de nouvelles opérations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f5231c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.sin(phase)\n",
    "print(\"y:\", y)\n",
    "print(\"y.grad_fn:\", y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5e2b6",
   "metadata": {},
   "source": [
    "Remarquez qu'ici seule la dernière opération est affichée.\n",
    "Il est cependant possible de remonter la chaîne des opérations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f80413",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"y.grad_fn:\", y.grad_fn)\n",
    "print(\"y.grad_fn.next_functions:\", y.grad_fn.next_functions)\n",
    "print(\"Next encore:\", y.grad_fn.next_functions[0][0].next_functions)\n",
    "print(\"Next encore (2x):\", y.grad_fn.next_functions[0][0].next_functions[0][0].next_functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98374c",
   "metadata": {},
   "source": [
    "Lorsqu'on atteint `AccumulatedGrad`, la chaîne est terminée (nous sommes revenus à `x`).\n",
    "\n",
    "Même les opérations qui ne sont pas directement de l'arithmétique, par exemple une copie avec `x.clone()`, peuvent être suivies par `autograd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2886132",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.clone())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db7b142",
   "metadata": {},
   "source": [
    "Par contre, les tenseurs qui utilisent `requires_grad` ne sont pas compatible avec Matplotlib et NumPy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b550c4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.title(\"Graphique avec PyTorch\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(\"Oups...\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7d2cef",
   "metadata": {},
   "source": [
    "Pour régler ce problème, il suffit de \"détacher\" les gradients.\n",
    "Cette fonction retourne un nouveau tenseur avec les mêmes valeurs, mais sans `requires_grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3040d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1714c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x.detach(), y.detach())\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Graphique avec PyTorch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732acc4",
   "metadata": {},
   "source": [
    "### Calcul des gradients\n",
    "\n",
    "Jusqu'à maintenant, nous avons emmagasiné les gradients via `requries_grad` et `grad_fn`, mais nous n'avons jamais demandé à PyTorch de calculer les gradients.\n",
    "Ainsi, l'attribut `grad` de nos tenseurs n'est pas défini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d75622",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1627dd24",
   "metadata": {},
   "source": [
    "Pour calculer les gradients, il suffit de prendre dernière sortie de notre chaîne d'opérations (ici `y`) et d'utiliser `.backward()`.\n",
    "Cette fonction exécutera la rétropropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201cbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward(torch.ones_like(y))  # Il faut passer le gradient initial quand y n'est pas scalaire, soit 1 ici (y vs y)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6548f3",
   "metadata": {},
   "source": [
    "Les gradients pour les \"feuilles\" de notre graphe de calcul sont ensuite accessible. Pour obtenir $\\frac{dy}{dx}$, on utilise donc `x.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5967530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"x.grad:\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252ad6bc",
   "metadata": {},
   "source": [
    "Par défaut, les gradients des étapes intermédiaires ne sont pas calculés:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a7ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"phase.grad\", phase.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3922f5d",
   "metadata": {},
   "source": [
    "Nous n'en aurons pas besoin, donc peut simplement ignorer le message d'avertissement.\n",
    "\n",
    "**Exercice: affichez sur un graphique y en fonction de x ainsi que la dérivée $\\frac{dy}{dx}$**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8ada8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Afficher le graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3253b655",
   "metadata": {},
   "source": [
    "## Créer un modèle PyTorch (Régression linéaire)\n",
    "\n",
    "Une des fonctionnalités très utile de PyTorch est son interface de modélisation `torch.nn`.\n",
    "Elle nous donne plusieurs blocs et fonctions utiles pour construire des réseaux neuronaux.\n",
    "Par contre, elle utilise la programmation orientée objet, donc la syntaxe est un peu différente des fonctions que nous utilisons habituellement.\n",
    "\n",
    "Une façon simple de se familiariser avec l'interface est d'implémenter une régression linéaire.\n",
    "\n",
    "### Données simulées\n",
    "\n",
    "Comme d'habitude, commençons par simuler des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf6af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 25\n",
    "noise_scale = 1.0\n",
    "w_true, b_true = 2.0, -1.0\n",
    "x = torch.linspace(-5, 5, steps=N)\n",
    "y_true = w_true * x + b_true\n",
    "y = y_true + noise_scale * torch.randn(N)\n",
    "\n",
    "plt.plot(x, y, \"kx\", label=\"Données simulées\")\n",
    "plt.plot(x, y_true, label=\"Vrai signal\", alpha=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba138d",
   "metadata": {},
   "source": [
    "### Définition du modèle\n",
    "\n",
    "Pour définir le modèle, il faut:\n",
    "\n",
    "- Créer une classe qui hérite de `nn.Module`, ici `LinearModel`.\n",
    "- Créer une méthode `__init__()`:\n",
    "    - C'est cette méthode qui sera appelée lorsqu'on crée un modèle avec `model = LinearModel()`\n",
    "    - La méthode `__init__` appelle la méthode `__init__()` de sa classe parent. On exécute ainsi le code que PyTorch implémente dans `nn.Module`.\n",
    "    - C'est généralement ici que l'on définira les couches de notre réseau\n",
    "- Créer une méthode `forward`. Cette méthode prend $x$ en entrée et exécute toute les couches de notre modèle.\n",
    "\n",
    "**Rappel: `self` désigne l'objet lui-même et permet d'assigner et d'accéder aux attributs de notre classe.**\n",
    "\n",
    "Dans notre cas, nous avons un modèle linéaire simple. Il faut donc une seule couche linéaire.\n",
    "On pourrait l'implémenter manuellement, mais PyTorch a un bloc `nn.Linear` qui définit exactement ce dont nous avons besoin!\n",
    "Selon [la documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html), `nn.Linear()` exécute\n",
    "$$\n",
    "y = x A^T + b.\n",
    "$$\n",
    "Tout ce que nous avons à faire, c'est de donner les dimensions d'entrée et de sortie. Les paramètre requis (poids et biais) seront créés automatiquement.\n",
    "Dans notre cas nous avons $f : \\mathbb{R} \\rightarrow \\mathbb{R}$, donc les dimensions sont de 1, le modèle sera donc équivalent à `y = a * x + b` où `a` et `b` sont des scalaires.\n",
    "Pour inclure une couche linéaire dans notre modèle, il suffit donc de 1) créer cette couche dans `__init__()` et 2) exécuter cette couche dans `forward()`.\n",
    "Ici, `self.linear` veut simplement dire qu'on emmagasine la couche linéaire dans une variable qui pourra être réutilisée ailleurs dans notre classe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17fff5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class LinearModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        # On appelle la fonction nn.Module.__init__()\n",
    "        # ceci permet au code d'initialisation de PyTorch de s'exécuter\n",
    "        super().__init__()\n",
    "        # On crée une couche linéaire pour l'utiliser dans forward\n",
    "        self.linear = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "model = LinearModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a109695e",
   "metadata": {},
   "source": [
    "Voilà, notre modèle est créé!\n",
    "\n",
    "Par défaut PyTorch initialise le modèle à des paramètres aléatoires.\n",
    "On peut accéder à ces paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273991c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pprint.pprint(list(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b440a23",
   "metadata": {},
   "source": [
    "On peut ensuite appeler notre modèle sur un point $x$ quelconque.\n",
    "PyTorch s'attend à recevoir un tenseur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65051c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(torch.tensor([0.0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332c537",
   "metadata": {},
   "source": [
    "Si on veut passer plusieurs points à la fois, il faut leur donner le format `(npts, ndim)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fa5a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = torch.tensor([[0.0, 1.0]]).T\n",
    "print(X_test.shape)\n",
    "model(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc91f18",
   "metadata": {},
   "source": [
    "Il faut donc formatter nos données `x` si on veut les passer dans notre modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8cd0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = x.unsqueeze(1)  # Ajoute une dimension à la position ndim, donc à la 2e dimension ici\n",
    "Y = y.unsqueeze(1)\n",
    "ypred = model(X)\n",
    "print(ypred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, \"kx\", label=\"Données simulées\")\n",
    "plt.plot(x, y_true, label=\"Vrai signal\", alpha=0.5)\n",
    "plt.plot(x, ypred.detach(), label=\"Prédiction initiale\", alpha=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582b7868",
   "metadata": {},
   "source": [
    "### Entraînement du modèle (optimisation)\n",
    "\n",
    "Les méthodes d'optimisation PyTorch assez différentes de celles fournies par SciPy.\n",
    "On les utilise typiquement de la façon suivante:\n",
    "\n",
    "- Définition d'une fonction objectif (_loss_)\n",
    "- Définition d'un optimiseur (ici `SGD`)\n",
    "- On crée manuellement une boucle dans laquelle:\n",
    "  - On calcule la fonction de objectif\n",
    "  - On propage les gradients\n",
    "  - On fait faire un pas à l'optimiseur\n",
    "  - On remet les gradients à 0 avant la prochaine itération\n",
    "\n",
    "C'est généralement une bonne idée d'emmagasiner ou d'afficher les valeurs de la fonction objectif pour vérifier son évolution.\n",
    "Si elle varie encore beaucoup, on peut ajouter des itérations.\n",
    "\n",
    "Ici, on utilise l'erreur carrée moyenne pour notre fonction objectif, soit\n",
    "\n",
    "$$\n",
    "\\mathrm{MSE} = \\frac{1}{N} \\sum_i^N (\\hat{y_i} - y_i)^2\n",
    "$$\n",
    "\n",
    "**Exercice: Complétez la fonction `loss_fn`. Assurez-vous d'utiliser `torch` pour calculer la moyenne.**\n",
    "\n",
    "**Exercice: Créez un tableau vide de taille `niter` pour y placer les valeurs de `loss`. Affichez ces valeurs après l'entraînement pour vérifier la convergence.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abbf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel()\n",
    "\n",
    "def loss_fn(y, ypred):\n",
    "    # TODO: Loss function\n",
    "\n",
    "# On donne les paramètres du modèle à SGD.\n",
    "# L'optimiseur peut ensuite se servir de leur gradient.\n",
    "optimizer = torch.optim.SGD(model.parameters())\n",
    "\n",
    "niter = 2000\n",
    "\n",
    "# TODO: Tableau loss_vals\n",
    "for i in range(niter):\n",
    "\n",
    "    # On calcule la prédiction du modèle\n",
    "    # et la fonction objectif\n",
    "    ypred = model(X)\n",
    "    loss = loss_fn(Y, ypred)\n",
    "\n",
    "\n",
    "    loss.backward()  # backprop\n",
    "    optimizer.step()  # un pas dans l'espace-paramètre, utilise le gradietn calculé par loss.backward()\n",
    "    optimizer.zero_grad()  # on réinitialise les gradients pour ne pas les acumuler entre les itérations\n",
    "\n",
    "    # TODO: Ajouter loss à loss_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80001d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphique loss vs itération"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77193673",
   "metadata": {},
   "source": [
    "Voilà, le modèle, est en entraîné!\n",
    "Notez que normalement on utiliserait une partie des données comme ensemble de test et qu'on testerait la généralisation du modèle.\n",
    "Nous en verrons un exemple au prochain cours.\n",
    "Ici, nous allons nous contenter de cette optimisation simple.\n",
    "\n",
    "### Prédictions avec le modèle entraîné\n",
    "\n",
    "Maintenant que le modèle est entraîné, vérifions la valeur des paramètres la prédiction du modèle.\n",
    "\n",
    "**Exercice: affichez la valeur des poids et un graphique montrant la prédiction du modèle.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e206b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Imprimer les poids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1bdfb87",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# TODO: Ajouter la prédiction optimisée au graphique\n",
    "plt.plot(x, Y, \"kx\", label=\"Données simulées\")\n",
    "plt.plot(x, y_true, label=\"Vrai signal\", alpha=0.5)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94784341",
   "metadata": {},
   "source": [
    "## Exercice: XOR\n",
    "\n",
    "Tel que vu en classe, l'opération XOR est une opération logique qui prend en entrée deux nombres ($x_1$, $x_2$) et qui retourne:\n",
    "\n",
    "- $y=1$ si l'un des deux nombres est 1, mais pas les deux\n",
    "- $y=0$ si les deux nombres sont 0 ou si les deux nombres sont 1\n",
    "\n",
    "On peut représenter cette situation avec un vecteur $x = [x_1, x_2]$ en entrée et un scalaire $y$ en sortie.\n",
    "\n",
    "Le graphique ci-dessous illustre le problème."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11facce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.tensor([[0.0, 0.0], [0.0, 1.0], [1.0, 0.0], [1.0, 1.0]])\n",
    "y = torch.tensor([0.0, 1.0, 1.0, 0.0])\n",
    "Y = y.unsqueeze(1)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=100, marker='x', label=\"Données\",cmap=\"coolwarm\")\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$x_2$\")\n",
    "plt.axis(\"square\")\n",
    "plt.title(\"Données pour l'opération XOR\")\n",
    "plt.colorbar(label=\"y\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1149fe3",
   "metadata": {},
   "source": [
    "Nous avons également vu en classe que l'opération XOR ne peut être modélisée par une régression linéaire, mais peut l'être par un petit réseau neuronal contenant une couche cachée de deux neurones.\n",
    "\n",
    "### Régression linéaire\n",
    "\n",
    "Commençons par vérifier que la régression linéaire se comporte comme nous avons vu en classe, c'est à dire qu'elle converge vers des poids à 0 et un biais à 0.5.\n",
    "\n",
    "**Exercice: copiez le modèle de régression linéaire ci-dessus, mais adaptez le pour accepter deux dimensions d'entrée (`in_features`)**\n",
    "**Initialisez ensuite le modèle et testez le sur les données `X`. Imprimez le tenseur retourné.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b801416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Régression linéaire 2D vers 1D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b932f757",
   "metadata": {},
   "source": [
    "**Imprimez les paramètres du modèle. Le format est-il celui auquel vous vous attendiez?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe56824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Imprimer les paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bda2180",
   "metadata": {},
   "source": [
    "**Copiez la boucle d'entraînement de la section précédente, mais adaptez-la à ce nouveau problème. Assurez-vous d'utiliser `Y` et non `y` pour l'entraînement! Utilisez un nombre d'itération suffisant pour que la fonction objectif semble avoir convergé à une valeur stable.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc2fa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfb7e6d",
   "metadata": {},
   "source": [
    "**Affichez maintenant les paramètres entraînés et la prédiction du modèle. Vous pouvez simplement l'imprimer ou la représenter sur un graphique.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a434e7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paramètres et prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0a7031",
   "metadata": {},
   "source": [
    "La prédiction donne le résultat vu en classe.\n",
    "\n",
    "### Réseau neuronal simple\n",
    "\n",
    "Essayons maintenant un réseau neuronal avec une couche cachée de deux neurones et une activation sigmoide.\n",
    "\n",
    "**Implémentez un réseau neuronal avec l'architecture suivante**:\n",
    "\n",
    "- Dans la fonction `__init__()`, définissez:\n",
    "    - Une couche linéaire avec 2 dimensions d'entrée ($x$) et deux dimensions de sortie ($z$)\n",
    "    - Une fonction d'activation sigmoide (`nn.Sigmoid`) qui donnera $h = g(z)$. Vous pouvez la créer avec `self.activation = nn.Sigmoid()`\n",
    "    - Une couche linéaire avec deux dimensions d'entrée ($h$) et une dimension de sortie ($y$)\n",
    "- Dans la fonction `forward()`, appelez les couches dans l'ordre et retournez la valeur y finale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66312a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class TinyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        # TODO: Fonction init\n",
    "    def forward(self, x):\n",
    "        # TODO: propagation entre les couches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1fd380",
   "metadata": {},
   "source": [
    "**Initialisez le modèle et affichez ses paramètres**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685d2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Modèle et paramètres"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f34d52",
   "metadata": {},
   "source": [
    "**Optimisez le modèle en complétant la boucle d'optimisation ci-dessous. Affichez ensuite la fonction objectif en fonction du temps.**\n",
    "\n",
    "Ici, l'optimiseur a été modifié pour augmenter le taux d'apprentissage (`lr` pour _learning rate_) et le momentum (que nous verrons dans les prochains cours).\n",
    "La surface d'optimisation est plus complexe et ces paramètres aident à ce que le modèle converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2793ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TinyNN()\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.02, momentum=0.9)\n",
    "\n",
    "# TODO: Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d62838f",
   "metadata": {},
   "source": [
    "**Affichez maintenant les paramètres entraînés et la prédiction du modèle. Vous pouvez simplement l'imprimer ou la représenter sur un graphique.**\n",
    "\n",
    "Si les résultats ne sont pas satisfaisants, retournez à l'étape d'entraînement et tentez d'ajouter des itérations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f62e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Paramètres et prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6111eb5c",
   "metadata": {},
   "source": [
    "**Utilisez `model.linear1` et `model.activation` pour obtenir les valeurs $h$ de la couche cachées. Affichez les valeurs de $y$ en fonction de $h_1$ et $h_2$. Pourquoi cette représentation améliore-t-elle la performance du modèle?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0e1aa9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# TODO: Afficher la représentation intermédiaire h"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
