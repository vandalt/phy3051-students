{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34fe1e03-41b7-48fb-a7c7-3e5d97d0ced0",
   "metadata": {},
   "source": [
    "# Réseaux neuronaux convolutifs\n",
    "\n",
    "Au dernier cours, nous avons vu comment construire un réseau neuronal simple et pleinement connecté ainsi que comment l'appliquer à la classification d'images.\n",
    "Nous avons aussi couvert comment entraîner ce réseau et l'appliquer à des données test, ou encore à des données provenant de sources externes.\n",
    "\n",
    "Or, le réseau pleinement connecté que nous utilisions n'était pas nécessairement le plus adapté  à la classification d'images. Pour ce genre de tâche un réseau neuronal convolutif est généralement plus approprié.\n",
    "Nous verrons aujourd'hui comment implémenter un tel réseau avec PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011c6aa2-cf06-49c7-a5ad-bd4829429cb4",
   "metadata": {},
   "source": [
    "## Données\n",
    "\n",
    "Nous allons commencer par appliquer notre modèle aux mêmes données qu'au dernier cours, soit l'ensemble FashionMNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4c6300-9b33-47ea-954f-b69ebbfbe06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"../cours03-images-mlp-pytorch/data\",  # On réutilise le même dossier pour ne pas télécharger en double\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),  # Transformation de tableau PIL vers tenseur\n",
    ")\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"../cours03-images-mlp-pytorch/data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "labels_map = {\n",
    "    0: \"T-Shirt\",\n",
    "    1: \"Trouser\",\n",
    "    2: \"Pullover\",\n",
    "    3: \"Dress\",\n",
    "    4: \"Coat\",\n",
    "    5: \"Sandal\",\n",
    "    6: \"Shirt\",\n",
    "    7: \"Sneaker\",\n",
    "    8: \"Bag\",\n",
    "    9: \"Ankle Boot\",\n",
    "}\n",
    "inv_labels_map = dict(zip(labels_map.values(), labels_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9a7d6f-0987-4d1d-8acd-d6c16360b6df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99423d5f-e091-40af-969f-42bff222f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_grid(data):\n",
    "    \"\"\"\n",
    "    Fonction pour tirer 9 images au hasard d'un Dataset PyTorch et les fafficher\n",
    "    \"\"\"\n",
    "    figure = plt.figure(figsize=(8, 8))\n",
    "    cols, rows = 3, 3\n",
    "    for i in range(1, cols * rows + 1):\n",
    "        # On tire une image au hasard et on l'affiche\n",
    "        sample_idx = torch.randint(len(data), size=(1,)).item()\n",
    "        img, label = data[sample_idx]\n",
    "        figure.add_subplot(rows, cols, i)\n",
    "        plt.title(labels_map[label])\n",
    "        plt.axis(\"off\")\n",
    "        plt.imshow(img.permute(1, 2, 0), cmap=\"binary\")\n",
    "    return figure\n",
    "    \n",
    "plot_grid(training_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da55596e-2e2c-4382-9bc7-bd0108768d65",
   "metadata": {},
   "source": [
    "## Définition d'un CNN\n",
    "\n",
    "### Couche de convolution\n",
    "\n",
    "Commençons par nous familiariser avec la couche de convolution 2D de PyTorch, soit [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html).\n",
    "\n",
    "**Exercices:**\n",
    "\n",
    "1. Le premier paramètre de `Conv2d` est le nombre de canaux en entrée. Combien de canaux d'entrée une couche de convolution utilisée sur les données Fashion MNST?\n",
    "2. Créez un modèle simple en assignant directement `model` à une couche de convolution, par exemple `model = nn.Conv2d(*args)` où `*args` est remplacé par les arguments requis. La couche de convolution devrait avoir le nombre de canaux d'entrée adéquat pour les données Fahsion MNIST et 8 canaux de sortie. Utilisez un noyaux de 5 pixels pour la convolution.\n",
    "3. Affichez les paramètres de ce modèle. Quel est le format (`shape`) des poids ?\n",
    "4. Passez une des images d'entraînement dans le modèle. Quelles sont les dimensions de sortie?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb93e84-af2f-4d1e-91ed-8d04d2f3eda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "model = None # TODO: Remplacez None par une couche de convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffacf20e-9544-4a5d-a64a-8ac633437fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utilisez .named_parameters() ou .paramters() pour accéder aux poids et afficher leur format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0b0e9c-fcca-40f2-a001-7cb2789ac0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Accès à une image et utilisation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e2c1ba-eebf-425d-bff3-f9b0ea61b09e",
   "metadata": {},
   "source": [
    "### Calcul des dimensions de sortie\n",
    "\n",
    "Pour une seule couche de convolution, il est facile de vérifier les dimensions de sortie. Pour se faciliter la vie, on peut également définir une fonction qui fait ce calcul. Pour un padding de type `same`, c'est assez facile: les dimensions de sortie sont égales à celles d'entrée. Or, pour un padding `valid`, ce ne sera pas le cas. De manière générale, les dimensions de sortie d'une convolution sont données par l'équation ci-dessous.\n",
    "\n",
    "Pour une image de hauteur (et largeur) $H_in$, la hauteur (et largeur) de la couche de sortie est donnée par:\n",
    "\n",
    "$$\n",
    "H_{out} = \\left\\lfloor\\frac{H_{in}  + 2 \\times \\text{padding} - \\text{dilation}\n",
    "                        \\times (\\text{kernel\\_size} - 1) - 1}{\\text{stride}} + 1\\right\\rfloor\n",
    "$$\n",
    "\n",
    "Pour nous, dans la plupart des cas:\n",
    "- `padding=0` car on utilise une convolution \"valid\" par défaut\n",
    "- `stride=1` car on glisse le noyau de convolution pixel par pixel\n",
    "- `dilation=1`: on ne dilate pas le noyau\n",
    "\n",
    "On peut voir intuitivement que:\n",
    "\n",
    "- Un plus padding plus grand augmente la dimension de sortie\n",
    "- Un `stride` (un pas) plus grand diminue les dimensions (on répète la convolution à moinds d'emplacements sur l'image)\n",
    "- Un noyau plus grand résulte en une dimension de sortie plus petite: plus les pixels du noyau couvrent une grande partie de l'image, moins on a de jeu pour glisser la convolution. C'est un peu la même idée avec la dilatation.\n",
    "\n",
    "**Exercice: Implémentez une fonction qui calcule cette équation et vérifiez son résultat avec la convolution ci-dessus.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f6a255-9b03-4edc-8de5-c46d3a7927ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_width(input_width, kernel_size, stride=1, padding=0, dilation=1):\n",
    "    \"\"\"\n",
    "    - input_wdith: taille de l'entrée\n",
    "    - kernel_size: taille du noyau\n",
    "    - stride: pas effectué par le noyau convolution\n",
    "    - padding: nombre de pixels de padding (0 pour valid, varie pour same)\n",
    "    - dilation: Espacement entre les pixels du noyau\n",
    "    # Ref: https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html#torch.nn.Conv2d\n",
    "    **Attention: stride=kernel_size pour max pooling par défaut**\n",
    "    \"\"\"\n",
    "    # TODO: Implémentation et test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de966ff7-a6d8-47ce-b872-390a76aea668",
   "metadata": {},
   "source": [
    "### Padding différent\n",
    "\n",
    "**Exercice: Utilisez un modèle alternatif avec un padding 'same'. Le format de la couche de sortie est-il de la manière attendue?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80857f0a-a226-465c-b162-911f9fc2e03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Convolution et test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafaac69-c0f7-4846-86c8-25da9cb6f95f",
   "metadata": {},
   "source": [
    "### Mise en commun (_pooling_)\n",
    "\n",
    "PyTorch implémente aussi des couches de mise en commun: [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html).\n",
    "\n",
    "**Exercice: Implémentez une couche de mise en commun de dimension 5 et utilisez-la sur une image d'entraînement. Quelles sont les dimensions de sortie? Comment diffèrent-elles d'une convolution? Est-ce cohérent avec l'équation vue plus haut? Affichez sur deux graphiques côte à côte l'image initiale et le résultat du pooling.**\n",
    "\n",
    "<details>\n",
    "    <summary>Explication concernant la taille de sortie</summary>\n",
    "    Par défaut, pour `MaxPool2d`, `stride=kernel_size`.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7c4350-15f2-4123-ad26-02406b96864e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pooling, utilisation et visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bdfbc69-ee6f-4d14-b89a-5899acd462bd",
   "metadata": {},
   "source": [
    "### Définition d'un CNN complet\n",
    "\n",
    "Pour définir un CNN, il suffit de combiner une ou plusieurs couches de convolution comme celle définie ci-dessus dans un réseau neuronal.\n",
    "\n",
    "**Exercice: Définissez un CNN avec les caractéristiques ci-dessous. Essayez d'abord de le faire sans retourner voir l'exemple du dernier cours, mais n'hésitez pas à vous y référer au besoin.**\n",
    "\n",
    "- Une première couche de convolution avec 6 canaux de sortie et un noyaux de largeur 5\n",
    "- Une deuxième couche de convolution avec 16 canaux de sortie et un noyaux de largeur 5\n",
    "- Trois couches pleinement connectées avec 120, 84 et 10 neurones\n",
    "- Une fonction d'activation ReLU pour toutes les couches sauf la dernière\n",
    "- Un pooling \"max\" 2x2 après l'activation des couches de convolution.\n",
    "- Dans l'exemple du dernier cours, nous avons utilisé `nn.Flatten` pour applatir les images. Nous en aurons besoin ici également. À quel endroit dans le réseau doit on applatir les données?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ba689e-9bd3-4482-a022-f45b4a194757",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Réseau CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8391f-b4b6-4aee-9525-09417f502b92",
   "metadata": {},
   "source": [
    "**Exercice: créez une instance de votre modèle. Quelle est la dimension d'entrée de la première couche pleinement connectée?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aabb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = ConvNet().to(device)\n",
    "# TODO: Accès à la dimension de la couche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2583f9-6fc3-4c88-8268-fc3a35b69d6e",
   "metadata": {},
   "source": [
    "### Test rapide du modèle\n",
    "\n",
    "**Exercice: Assurez-vous que votre modèle fonctionne sur une image tirée de `training_data` et un sous-ensemble tiré de `train_dataloader`.**\n",
    "Pour l'image seule, n'oubliez pas d'ajouter la dimension de \"batch\" avec `img.unsqueeze()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adedb27-aea5-43f2-9d8f-4155b2484ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Test du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ef3e7a-9cc9-4d53-9224-b07860b14ff5",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "\n",
    "**Exercie: Entraînez le réseau avec une fonction objectif _cross-entropy_.**\n",
    "\n",
    "Essayez de vous référer à l'exemple du dernier cours le moins possible, mais consultez le au besoin.\n",
    "\n",
    "Rappel des étapes:\n",
    "\n",
    "- Définition de la fonction objectif et de l'optimiseur\n",
    "- Définition d'une boucle d'entraînement et d'une boucle de test\n",
    "- Itération sur les époques (commencez par 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9c5b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Fonction objectif et optimiseur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad108d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Boucle d'entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e098a36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Boucle de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25c08c2-8adb-40bd-847f-20c19b863d8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO: Entraînement par époque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e45b70-b27a-41ef-b978-2c3bb8e671fd",
   "metadata": {},
   "source": [
    "**Exercice: Affichez l'évolution de la fonciton objectif pour les données d'entraînement et de test.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d145ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Graphique"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee356932-5ba7-4ecb-a82f-afb296940138",
   "metadata": {},
   "source": [
    "Remarquez que la performance après 10 époques est moins bonne que pour le réseau pleinement connecté du dernier cours! Ceci s'explique en partie le fait que notre réseau est très simple et en partie par le fait que nous avons coupé l'entraînement relativement tôt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa445133-cd34-4394-9b1d-f178fe82151f",
   "metadata": {},
   "source": [
    "## Inspection du modèle\n",
    "\n",
    "Comme au dernier cours, on pourrait utiliser le modèle entraîner pour prédire différents exemples. Le code serait pratiquement le même.\n",
    "\n",
    "Par contre, comme nous avons un CNN ici, on peut essayer d'interpréter les poids dans les différentes convolutions.\n",
    "\n",
    "**Exercice: Accédez à la première couche de convolution. Ensuite, accédez à ses poids et affichez le poids pour les différents canaux.**\n",
    "\n",
    "<details>\n",
    "    <summary>Indice</summary>\n",
    "    Il faudra d'abord accéder à `conv_stack` et `conv_stack` est comme une liste, donc on peut l'indexer.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a12bc40-2b69-4196-bf2b-9b0d5b21c56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Accès à la couche de convolution et affichage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a3cdb8-6187-420f-a976-069f36cd21b4",
   "metadata": {},
   "source": [
    "Assez difficile d'interpréter quoi que ce soit ici!\n",
    "\n",
    "**Exercice: Affichez maintenant les sorties de la première couche pour une image tirée au hasard des données d'entraînement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477cfde8-fe82-4a80-bc98-8fc5fcbf01dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utilisation de la couche de convolution et affichage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d6378b-d925-448f-8068-5450d70d045b",
   "metadata": {},
   "source": [
    "## Exercices additionnels\n",
    "\n",
    "- Modifiez le réseau ci-dessus en ajoutant des canaux, des couches et/ou en modifiant la taille des noyaux de convolutions et explorez l'effet sur le résultat.\n",
    "- Modifiez le réseau ci-dessus afin qu'il accepte des images RGB avec 3 canaux et testez le sur les données CIFAR-10 (` torchvision.datasets.CIFAR10`)\n",
    "- Utilisez un \"dropout\" dans le modèle\n",
    "- Utilisez une \"Batch normalization\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phy3051",
   "language": "python",
   "name": "phy3051"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
