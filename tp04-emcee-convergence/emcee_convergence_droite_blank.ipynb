{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2df4b25e-b51c-4343-b402-ade594a666a1",
   "metadata": {},
   "source": [
    "# _emcee_ et convergence\n",
    "\n",
    "Au dernier TP, nous avons implémenté un MCMC Metropolis-Hastings nous-mêmes. En pratique, il est généralement plus simple et plus robuste d'utiliser une librairie bien testée et qui permet d'explorer différentes distributions de proposition. Pour le reste du cours, nous ferons les MCMCs avec [_emcee_](https://emcee.readthedocs.io/en/stable/). Par défaut, _emcee_ utilise le _stretch move_ que nous avons vu en classe pour proposer des échantillons.\n",
    "\n",
    "En plus d'implémenter un échantillonneur (_sampler_), _emcee_ inclut également des fonctions pour vérifier la convergence de nos chaînes. D'ailleurs, les diapositives sur l'autocorrélation sont inspirées par le tutoriel  [Autocorrelation analysis & convergence](https://emcee.readthedocs.io/en/stable/tutorials/autocorr/) d'_emcee_.\n",
    "\n",
    "Dans ce TP, nous allons apprendre à utiliser _emcee voir en pratique comment certains facteurs affectent la convergence. Nous utiliserons la fonction `get_autocorr_time()` d'_emcee_ pour calculer le temps d'autocorrélation de nos chaînes. Cette fonction nous avertira également si nos chaînes sont moins longues que 50 fois la valeur estimée du temps d'autocorrélation.\n",
    "\n",
    "\n",
    "Plan:\n",
    "1. [MCMC appliqué à l'analyse de données](#section-1)\n",
    "1. [Distributions de proposition et convergence](#section-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634777da-55fd-4c4a-82d0-1a27f61d2221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import corner\n",
    "import emcee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678bcdb-b22c-4d6e-9692-68524c72c5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams[\"font.size\"] = 18\n",
    "rcParams[\"figure.figsize\"] = (9, 6)\n",
    "rcParams[\"xtick.direction\"] = \"in\"\n",
    "rcParams[\"ytick.right\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473c6f14-2db6-4524-b67b-ce17b1f4aead",
   "metadata": {},
   "source": [
    "<a id=\"section-1\"></a>\n",
    "## 1 - MCMC appliqué à l'analyse de données\n",
    "\n",
    "Dans cette section, nous allons appliquer _emcee_ pour modéliser un signal périodique.\n",
    "\n",
    "### Données synthétiques et vraisemblance maximale\n",
    "\n",
    "Les première étapes consistent à générer des données synthétiques autour d'une droite, à estimer les paramètres ayant une vraisemblance (sans les _priors_) maximale et à les utiliser comme point de départ dans un MCMC qui échantillone la distribution à posteriori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7617d249-b29f-4f77-902d-9b04093d97cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cette cellulle est tirée du tutoriel emcee pour obtenir exactement les mêmes données\n",
    "np.random.seed(123)\n",
    "\n",
    "# Choose the \"true\" parameters.\n",
    "m_true = -0.9594\n",
    "b_true = 4.294\n",
    "f_true = 0.534\n",
    "\n",
    "# Generate some synthetic data from the model.\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 0.5 * np.random.rand(N)\n",
    "y = m_true * x + b_true\n",
    "y += np.abs(f_true * y) * np.random.randn(N)\n",
    "y += yerr * np.random.randn(N)\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0, label=\"Données simulées\")\n",
    "x0 = np.linspace(0, 10, 500)\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"Vrai signal\")\n",
    "plt.legend()\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4d5e4e-ac88-475f-b8ab-4a18e03dfb8d",
   "metadata": {},
   "source": [
    "Nous aurons besoin de la fonction de vraisemblance pour le premier estimé. Elle pourra être réutilisée plus tard pour définir la distribution à posteriori.\n",
    "\n",
    "Notre modèle physique est donné par une droite\n",
    "\n",
    "$$\n",
    "f(x) = m x + b.\n",
    "$$\n",
    "\n",
    "Nous utiliserons une vraisemblance gaussienne, mais ajouterons un paramètre additionnel au cas où les erreurs aient été sous-estimées, de sorte que la variance totale est donnée par:\n",
    "\n",
    "$$\n",
    "s_n^2 = \\sigma_n^2 + (m x_n + b)^2 f^2\n",
    "$$\n",
    "\n",
    "où $\\sigma_n^2$ correspond à `yerr` pour chaque point. Il est généralement suggéré d'utiliser un tel facteur ou, encore plus simple, un paramètre $sigma^2$ tel que $s_n^2 = \\sigma_n^2 + \\sigma^2$ pour s'assurer que les barres d'erreur ne sont pas sous-estimées. Si elles ne l'étaient pas, le paramètre convergera simplement vers 0.\n",
    "\n",
    "**Exercice: Implémentez la fonction de log-vraisemblance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27062d46-9ec6-4aed-8d64-7352ae7e2049",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x, y, yerr):\n",
    "    \"\"\"\n",
    "    - theta: vecteur de paramètres m, b et log_f\n",
    "    - x, y, yerr: données\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b1ba7-7dec-4de0-9f5f-e848cc51ba82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "np.random.seed(42)\n",
    "nll = lambda *args: -log_likelihood(*args)\n",
    "initial = np.array([m_true, b_true, np.log(f_true)]) + 0.1 * np.random.randn(3)\n",
    "soln = minimize(nll, initial, args=(x, y, yerr))\n",
    "m_ml, b_ml, log_f_ml = soln.x\n",
    "\n",
    "print(\"Maximum likelihood estimates:\")\n",
    "print(\"m = {0:.3f}\".format(m_ml))\n",
    "print(\"b = {0:.3f}\".format(b_ml))\n",
    "print(\"f = {0:.3f}\".format(np.exp(log_f_ml)))\n",
    "\n",
    "plt.errorbar(x, y, yerr=yerr, fmt=\".k\", capsize=0, label=\"Données\")\n",
    "plt.plot(x0, m_true * x0 + b_true, \"k\", alpha=0.3, lw=3, label=\"Vrai signal\")\n",
    "plt.plot(x0, np.dot(np.vander(x0, 2), [m_ml, b_ml]), \":k\", label=\"Estimé vraisemblance maximale\")\n",
    "plt.legend(fontsize=14)\n",
    "plt.xlim(0, 10)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9680a848-26a8-48b8-98d4-f364dbc75d5b",
   "metadata": {},
   "source": [
    "On peut voir que le modèle de vraisemblance maximale est déjà en bon accord avec le la vraie droite. Par contre, il n'y a pas de barres d'erreurs sur nos paramètres et nous n'avons specifiés aucuns _priors_.\n",
    "\n",
    "### Exemple initial de MCMC\n",
    "\n",
    "On peut d'abord reproduire l'exemple _emcee_ avec les mêmes _priors_ pour s'assurer que tout fonctionne comme prévu.\n",
    "On implémente tout en espace log, car c'est ce qu'utilise _emcee_, pour les raison numériques dont nous avons discuté en classe.\n",
    "\n",
    "Remarquez qu'on ne calcule pas le _likelihood_ si le _prior_ est 0 (log-prior $ = -\\infty$), pour sauver du temps de calcul.\n",
    "\n",
    "**Exercice: Implémentez la fonction log-prior et log-posterior**\n",
    "**Utilisez un prior entre -5 et 5 pour m, 0 et 10 pour b et -10 et 1 pour log(f)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd079ff-8923-4eec-b78c-dbf2224ae58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    pass\n",
    "\n",
    "def log_probability(theta, x, y, yerr):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1ced30-e2a2-4cef-82bd-0eea87c21f61",
   "metadata": {},
   "source": [
    "On peut maintenant échantilloner avec _emcee_. Quelques points importants:\n",
    "\n",
    "- _emcee_ échantillonne en parallèle avec plusieurs chaînes (32 ici).\n",
    "- On doit donc initialiser des \"marcheurs\" dans un tableau avec 32 positions aléatoires pour chacun de nos 3 paramètres.\n",
    "- Pour l'initialisation, généralement, une petite \"balle\" gaussienne suffit. Les marcheurs vont s'étendre à l'espace des paramètres assez rapidement.\n",
    "- Il y a une phase de _burn in_ ou _warm-up_ au début qui permet aux marcheurs de trouver l'ensemble typique du _posterior_.\n",
    "\n",
    "**Exercice: implémentez un MCMC avec emcee et exécutez le pour 5000 pas. Démarrez au point de vraisemblance maximale. Affichez ensuite l'évolution des chaînes, un _corner_ plot et calculez le temps d'autocorrélation.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84225d8-090e-432b-bf07-e5a135a435bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "627e1ed2-dcf7-4c32-bc87-35f9f4c1b838",
   "metadata": {},
   "source": [
    "**Exercice: calculez les incertitudes données par corner manuellement pour les vérifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df04101-b6da-4c0c-ae49-8dc3bd018aba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e44c7a1f-1896-47ee-9fec-d9c9d371d24b",
   "metadata": {},
   "source": [
    "On peut voir que $m$ et $b$ sont fortement anti-corrélés. Un changement dans l'un affectera fortement l'autre dans le MCMC et dans la solution finale\n",
    "\n",
    "Pour bien vérifier que le modèle correspond aux données, on peut également afficher des échantillons du MCMC superposés aux données.\n",
    "\n",
    "**Exercice: affichez le modèle pour 100 échantillons de votre MCMC avec les données et le vrai signal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f37a75e-f567-4d2e-9761-2688b5963c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b1d9569-b3b4-42bc-9758-a2528dfed9ca",
   "metadata": {},
   "source": [
    "### Effet de la longueur des chaines\n",
    "\n",
    "Si on répète le processus, mais qu'on s'arrête avec des chaînes trop courtes, l'échantillonage sera incomplet. _emcee_ nous le laissera savoir lorsqu'on calcule le temps d'autocorrélation, mais l'inspection visuelle nous aidera aussi.\n",
    "\n",
    "**Exercice: Répétez l'analyse ci-dessus mais avec des chaînes de 500 pas seulement.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57746b-59ad-4622-9c4b-f1d4197321f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3528974-bfca-4732-bf8b-76dbc402bee1",
   "metadata": {},
   "source": [
    "Ici, on peut utiliser l'option `quiet=True` pour avoir un avertissemment au lieu d'une erreur. Le fait que l'on puisse distinguer des variations claires dans les chaînes sur une échelle inférieure à ~ 1/50 de notre chaîne est un autre indicateur (quoique beaucoup moins rigoureux).\n",
    "\n",
    "Dans ce cas-ci, nous aurions donc eu besoin de plus de pas pour atteindre la convergence (mais le MCMC semblait sur la bonne voie)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd928d23-c051-4bd9-916d-6b1d8a77cb89",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Effet de la distribution à priori\n",
    "\n",
    "La distribution à priori (le _prior_) peut aussi avoir un impact sur la convergence.\n",
    "\n",
    "#### Prior trop étroit\n",
    "\n",
    "Un prior trop étroit sur un ou plusieurs paramètres peut restreindre les valeurs explorées et nous mener à sous-estimer les incertitudes. Dans un tel cas, il n'y a pas un problème de convergence à proprement parler, car la distribution peut être bien échantillonée quand même. Le problème est plutôt dans le _choix de distribution_.\n",
    "\n",
    "**Exercice: Utilisez un _prior_ sur $m$ est entre -1.1 et -0.9 et répétez les diagnotics et analyses ci-dessus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3132c2ff-6665-4122-8b43-04ea1eb57239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5449c442-8a0a-4793-a064-dc683ba89bee",
   "metadata": {},
   "source": [
    "L'effet décrit plus haut est aussi observable ici. Les bords de la distribution en $m$ sont coupés par le prior. A moins d'avoir une bonne raison de forcer le paramètre dans cette intervalle (par exemple une contrainte physique), il serait préférable d'élargir le prior.\n",
    "\n",
    "On peut aussi remarquer que le paramètre $b$ a lui aussi une forme plus coupée, en raison de la corrélation entre $m$ et $b$ (en limitant $m$, on limite $b$ du même coup).\n",
    "\n",
    "#### Prior qui exclue la solution\n",
    "\n",
    "En définissant un prior qui exclue la solution, nous forcerons $m$ vers des valeurs qui ne représentent pas la meilleure solution.\n",
    "\n",
    "**Exercice: Testez un prior qui exclue la solution. Quel est l'impact sur la distribution à posterior?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1c6e9-a957-43ff-889a-b0791c81f5c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e65b2079-ceca-4e5b-8540-7e4574714794",
   "metadata": {
    "tags": []
   },
   "source": [
    "Avec les deux graphiques ci-dessus, on voit encore que la pente n'est pas du tout en accord avec les données. Il serait préférable de reconsidérer notre _prior_ pour s'assurer qu'il a du sens physiquement et qu'il permet plus de valeurs en $m$.\n",
    "\n",
    "### Effet de la position de départ des marcheurs\n",
    "\n",
    "En changeant la position de départ des marcheurs, la forme des chaînes peut changer. Surtout pendant le _burn-in_. S'il y a des bi-modalités, elles pourraient également être révélées. En général, c'est une bonne idée de répéter le MCMC avec  différentes valeurs initiales pour voir s'il y a toujours une convergence vers la même forme de distribution.\n",
    "\n",
    "#### Dans le prior, mais plus loin du mode\n",
    "\n",
    "Dans les sections précédentes, notre initialisation était très près du mode de la distribution. On peut tester des valeurs volontairement plus éloignées pour voir si on retrouve la même solution.\n",
    "\n",
    "**Exercice: Testez des valeurs initiales différentes pour les paramètres (exemple `[-1.5, 2.0, -1.0]`)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10efd8-381f-4351-89e0-38677b0b7d68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "404f684c-0095-4973-8f24-96721901364d",
   "metadata": {},
   "source": [
    "#### Valeur initiale hors du prior\n",
    "\n",
    "Il peut arriver, par inattention, qu'on initialise les marcheurs hors du _prior_. C'est habituellement assez facile à identifier.\n",
    "\n",
    "Quelques exemples de solutions possibles:\n",
    "\n",
    "- Changer manuellement les valeurs initiales.\n",
    "- Optimiser le maximum à posteriori (MAP) au lieu de la vraisemblance maximale (ainsi notre valeur initiale tient compte des priors)\n",
    "- Réviser nos priors, l'erreur pourrait venir d'une borne mal définie et non de la position initiale des marcheurs.\n",
    "- Ajouter une vérification pour s'assurer que nos valeurs initiales donnent une valeur de _prior_ finie avant de commencer le MCMC (ça ne règle pas le problème, mais ça le détecte plus clairement).\n",
    "\n",
    "Voici un exemple de macheurs qui démarrent hors du _prior_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6871570-e009-479d-bbe0-4bcda204ff8b",
   "metadata": {},
   "source": [
    "Les chaînes sont droites car le _prior_ (et le _posterior_ ensuite) retourne $-\\infty$. La probabilité ne change donc jamais et tous les pas sont refusés. Dans un tel cas, l'autocorrélation sera indéfiniie (`nan`). On voit aussi que le _corner plot_ n'affiche pas de distribution claire, mais plus tôt les positions initiales des marcheurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97beb7-3e57-43b9-8f9e-bc163de97dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88daa42b-3f43-428c-821b-142a7ed9f21b",
   "metadata": {},
   "source": [
    "<a id=\"section-2\"></a>\n",
    "## 2 - Distributions de proposition et convergence\n",
    "\n",
    "Dans la prochaine section, nous explorerons des distributions simples pour voir l'effet de la distribution de proposition (_moves_ dans _emcee_) sur la convergence.\n",
    "\n",
    "Commençons avec une distribution bimodale en 1D (cette distribution est la même que dans le tutoriel \"[Using different moves](https://emcee.readthedocs.io/en/stable/tutorials/moves/)\" d'_emcee_). Ici, au lieu d'échantillonner une distribution postérieure pour déterminer les paramètres d'un modèle, nous allons seulement tirer des échantillons d'une distribution connue afin de comparer la convergence de différentes méthodes.\n",
    "\n",
    "Nous utiliserons trois distributions de proposition différente:\n",
    "1. Une distribution Gaussienne avec le Metropolis-Hastings (`emcee.moves.Gaussian()`)\n",
    "2. \"Stretch move\" (`emcee.moves.Stretch()`, la distribution par défaut d'_emcee_)\n",
    "3. MCMC par évolution différentielle (`emcee.moves.DEMove()` et `emcee.moves.DESnookerMove()`)\n",
    "\n",
    "Ces distributions sont déjà implémentées dans _emcee_ et peuvent être spécifiées au moment de créer notre \"_sampler_\" (objet `emcee.EnsembleSampler`).\n",
    "\n",
    "Nous pouvons d'abord définir notre distribution et, comme c'est une distribution facile à calculer analytiquement, nous pouvons l'afficher directement (ce n'est habituellement pas possible avec les distributions à posteriori que nous utilisons en physique ou en analyse de données)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0149c1f-c0ec-487f-a2a4-d917bf29e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logprob_bimod(x):\n",
    "    return np.sum(\n",
    "        np.logaddexp(-0.5 * (x - 2) ** 2, -0.5 * (x + 2) ** 2)\n",
    "        - 0.5 * np.log(2 * np.pi)\n",
    "        - np.log(2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a05de9-06ee-48eb-9fc0-c6f6ed3c2202",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-6, 6, num=1000)\n",
    "y = np.exp(list(map(logprob_bimod, x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb83a23-1d91-4b5c-8d22-153115841137",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, \"k\")\n",
    "plt.yticks([])\n",
    "plt.xlim(-5.5, 5.5)\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202324b6-134f-474e-a230-51a00811e266",
   "metadata": {},
   "source": [
    "### Metropolis-Hastings\n",
    "\n",
    "Nous allons d'abord utiliser l'algorithme de Metropolis-Hastings avec une distribution de proposition gaussienne. \n",
    "\n",
    "Remarquez que le paramètre `cov` (qui n'est qu'une variance dans ce cas-ci, en 1D) peut être ajusté. Une distribution bimodale est un exemple de cas où il peut être avantageux d'utiliser une covariance un peu plus élevée. Avoir une échelle plus grande permet de plus efficacement se déplacer entre les modes de la distribution. Vous pouvez vous amuser à varier ce paramètres et voir l'effet sur le temps d'autocorrélation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a08beb4-8741-40bb-b5a2-5c276aa371fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = np.random.randn(32, 1)\n",
    "nwalkers, ndim = init.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, logprob_bimod, moves=emcee.moves.GaussianMove(cov=1.0)\n",
    ")\n",
    "sampler.run_mcmc(init, 5000, progress=True)\n",
    "\n",
    "fchain_gauss = sampler.get_chain(flat=True)\n",
    "\n",
    "print(f\"Temps d'autocorrélation intégré, Metropolis-Hastings: {sampler.get_autocorr_time()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9b15c9-fe87-4244-b8c6-70ea1954c65e",
   "metadata": {},
   "source": [
    "On peut ensuite afficher un seul marcheur pour voir les détails de l'échantillonage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff3738-caa8-4e9c-9a54-30bb33591059",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.get_chain()[:, 0, 0], \"k\", lw=0.5)\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(-5.5, 5.5)\n",
    "plt.title(\"move: GaussianMove\", fontsize=20)\n",
    "plt.xlabel(\"step number\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45378a0d-ad43-4087-9894-547da6d281d7",
   "metadata": {},
   "source": [
    "### \"_Stretch Move_\" par défaut d'_emcee_\n",
    "\n",
    "Maintenant, nous pouvons tester le \"_stretch move_\" d'_emcee_, qui est utilisé par défaut. Il utilise la position des autre marcheurs pour orienter les pas, donc certaines distributions (avec des covariance, par exemple) peuvent être mieux échantillonées.\n",
    "\n",
    "Comme pour la variance du Metropolis-Hastings, l'échelle `a` pour le \"Stretch\" peut être ajustée. La valeur par défaut est `a=2.0`. Ici, une valeur de `3.0` semble améliorer l'efficacité (temps d'autocorrélation plus faible)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549a012e-758a-48f3-915c-3e4bb9e1ca89",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = np.random.randn(32, 1)\n",
    "nwalkers, ndim = init.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(\n",
    "    nwalkers, ndim, logprob_bimod, moves=emcee.moves.StretchMove(a=3.0)\n",
    ")\n",
    "sampler.run_mcmc(init, 5000, progress=True);\n",
    "\n",
    "fchain_stretch = sampler.get_chain(flat=True)\n",
    "\n",
    "print(f\"Temps d'autocorrélation intégré, Stretch: {sampler.get_autocorr_time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d65f9c-af1d-41e2-bafc-f423442fede4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.get_chain()[:, 0, 0], \"k\", lw=0.5)\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(-5.5, 5.5)\n",
    "plt.title(\"move: StretchMove\", fontsize=20)\n",
    "plt.xlabel(\"step number\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a223725-c7cb-4935-84f2-3d5610bf6e5b",
   "metadata": {},
   "source": [
    "On peut voir ci-dessus que le temps d'autocorrélation est plus court que pour le Metropolis. Donc l'échantillonage est plus efficace (il faudra moins de pas pour atteindre $N \\geq \\tau_f$).\n",
    "\n",
    "### MCMC par évolution différentielle\n",
    "\n",
    "Certaines méthodes pour proposer de nouveaux pas dans _emcee_ utilisent l'évolution différentielle. Je laisse un lien vers la page [Moves](https://emcee.readthedocs.io/en/stable/user/moves/) de la documentation pour les intéressé·e·s. Dans les cas légèrement bimodaux comme celui-ci. le MCMC par évolution différentielle peut être plus efficace.\n",
    "\n",
    "Il y a deux type de propositions par évolution différentielle (`DEMove()` et `DESnookerMove()`), donc nous allons combiner les deux en tirant 80 % des pas avec le premier et 20 % des pas avec le deuxième."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1053c608-44fb-4275-ac21-e45f30dd7cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "init = np.random.randn(32, 1)\n",
    "nwalkers, ndim = init.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, logprob_bimod, moves=[\n",
    "        (emcee.moves.DEMove(), 0.8),\n",
    "        (emcee.moves.DESnookerMove(), 0.2),\n",
    "    ])\n",
    "sampler.run_mcmc(init, 5000, progress=True)\n",
    "\n",
    "fchain_de = sampler.get_chain(flat=True)\n",
    "\n",
    "print(f\"Temps d'autocorrélation intégré, DE-MCMC: {sampler.get_autocorr_time()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e34f59-ebdd-4769-835e-57e7fb1a46e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(sampler.get_chain()[:, 0, 0], \"k\", lw=0.5)\n",
    "plt.xlim(0, 5000)\n",
    "plt.ylim(-5.5, 5.5)\n",
    "plt.title(\"move: DEMove et DESnookerMove\", fontsize=20)\n",
    "plt.xlabel(\"step number\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f8ec0e-9a3b-4061-8f9d-a1d68e4eff25",
   "metadata": {},
   "source": [
    "On voit que le temps d'autocorrélation est beaucoup plus petit que pour les deux premières distributions de proposition. Même à l'oeil, on voit que le marcheur explore plus rapidement.\n",
    "\n",
    "En général. la configuration par défaut d'_emcee_ (i.e. sans spécifier de `move` au `EnsembleSampler()`) fonctionne très bien. Par contre, si le temps de convergence est très long. Il peut être pertinent de varier les paramètres et le type de pas pour voir l'effet sur la performance du MCMC.\n",
    "\n",
    "Dans ce cas-ci, on peut voir que les 3 méthodes représentent bien la distribution analytique, malgré leurs différents temps de convergence. Ce ne sera pas toujours le cas avec les distributions à plusieurs dimensions auxquelles nous faisons généralement face en analyse de données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944ce72e-42b6-4a68-ad96-85d9aa5ce49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fchain_gauss, bins=50, density=True, histtype=\"step\", label=\"M-H gaussien\")\n",
    "plt.hist(fchain_stretch, bins=50, density=True, histtype=\"step\", label=\"Stretch\")\n",
    "plt.hist(fchain_de, bins=50, density=True, histtype=\"step\", label=\"DE-MCMC\")\n",
    "plt.plot(x, y, \"k\", label=\"Courbe analytique\")\n",
    "plt.yticks([])\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
